{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/flair\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/flair\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install elasticsearch elasticsearch_dsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$1': {'$1': 2}}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk = \"$1\"\n",
    "a = {tk: {tk: 1}}\n",
    "a[tk][tk] += 1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aaa': {'co_tickers': {'$1': 2, '$2': 2, '$3': 2},\n",
       "  'co_keywords': {'aaa': 2, 'bbb': 2, 'ccc': 2}},\n",
       " 'bbb': {'co_tickers': {'$1': 2, '$2': 2, '$3': 2},\n",
       "  'co_keywords': {'aaa': 2, 'bbb': 2, 'ccc': 2}},\n",
       " 'ccc': {'co_tickers': {'$1': 2, '$2': 2, '$3': 2},\n",
       "  'co_keywords': {'aaa': 2, 'bbb': 2, 'ccc': 2}}}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers = [\"$1\", \"$2\", \"$3\"]\n",
    "keywords = [\"aaa\", \"bbb\", \"ccc\"]\n",
    "\n",
    "ticker_name = {}\n",
    "ticker_count = {}\n",
    "keyword_count = {}\n",
    "\n",
    "def add(tickers, keywords):\n",
    "    for tk in tickers:\n",
    "        if tk not in ticker_count:\n",
    "            ticker_count[tk] = {\"co_tickers\": {}, \"co_keywords\": {}}\n",
    "        for ctk in tickers:\n",
    "            try:\n",
    "                ticker_count[tk][\"co_tickers\"][ctk] += 1\n",
    "            except KeyError:\n",
    "                ticker_count[tk][\"co_tickers\"][ctk] = 1\n",
    "        for ckw in keywords:\n",
    "            try:\n",
    "                ticker_count[tk][\"co_keywords\"][ckw] += 1\n",
    "            except KeyError:\n",
    "                ticker_count[tk][\"co_keywords\"][ckw] = 1\n",
    "\n",
    "    for kw in keywords:\n",
    "        if kw not in keyword_count:\n",
    "            keyword_count[kw] = {\"co_tickers\": {}, \"co_keywords\": {}}\n",
    "        for ctk in tickers:\n",
    "            try:\n",
    "                keyword_count[kw][\"co_tickers\"][ctk] += 1\n",
    "            except KeyError:\n",
    "                keyword_count[kw][\"co_tickers\"][ctk] = 1\n",
    "        for ckw in keywords:\n",
    "            try:\n",
    "                keyword_count[kw][\"co_keywords\"][ckw] += 1\n",
    "            except KeyError:\n",
    "                keyword_count[kw][\"co_keywords\"][ckw] = 1\n",
    "\n",
    "add(tickers, keywords)\n",
    "add(tickers, keywords)\n",
    "\n",
    "ticker_count\n",
    "keyword_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/flair/app/app\n",
      "{'keywords': ['JPMorgan Chase & Co',\n",
      "              'Amazon.com Inc',\n",
      "              'Stock markets',\n",
      "              'Ted Weschler',\n",
      "              'Todd Combs',\n",
      "              'Bank of America Corp',\n",
      "              'Apple Inc',\n",
      "              'American Express Co',\n",
      "              'S&P 500 Index',\n",
      "              'Warren Buffett',\n",
      "              'Berkshire Hathaway Inc',\n",
      "              'Markets',\n",
      "              'Investment strategy',\n",
      "              'business news'],\n",
      " 'tickers': [{'labels': [['Berkshire Hathaway', 'BRK.A']],\n",
      "              'text': 'Billionaire investor Warren Buffett told CNBC on Monday '\n",
      "                      'that Berkshire Hathaway investment managers Ted '\n",
      "                      'Weschler and Todd Combs have trailed the   by a \"tiny '\n",
      "                      'bit\" since each joined the company.'},\n",
      "             {'labels': [['American Express', 'AXP'],\n",
      "                         ['Apple', 'AAPL'],\n",
      "                         ['Bank of America', 'BAC']],\n",
      "              'text': \"Berkshire's portfolio of stocks, including top holdings \"\n",
      "                      'American Express, Apple\\xa0and Bank of America, was '\n",
      "                      'valued at nearly $173 billion at the end of 2018.'},\n",
      "             {'labels': [['Amazon', 'AMZN'], ['J.P. Morgan', 'JPM']],\n",
      "              'text': '\"Both of them have done an incredible amount of work in '\n",
      "                      'terms of acquisitions; Todd, in particular, on our '\n",
      "                      'medical venture,\" said Buffett, referring to the '\n",
      "                      'Berkshire-Amazon-J.P. Morgan venture aimed at figuring '\n",
      "                      'out how to reduce health-care costs for their employees '\n",
      "                      'and how that may possibly help cut health-care costs in '\n",
      "                      'the United States.'}]}\n",
      "['BRK.A', 'AXP', 'AAPL', 'BAC', 'AMZN', 'JPM']\n",
      "['JPMorgan Chase & Co', 'Amazon.com Inc', 'Stock markets', 'Ted Weschler', 'Todd Combs', 'Bank of America Corp', 'Apple Inc', 'American Express Co', 'S&P 500 Index', 'Warren Buffett', 'Berkshire Hathaway Inc', 'Markets', 'Investment strategy', 'business news']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BRK.A': {'Berkshire Hathaway'},\n",
       " 'AXP': {'American Express'},\n",
       " 'AAPL': {'Apple'},\n",
       " 'BAC': {'Bank of America'},\n",
       " 'AMZN': {'Amazon'},\n",
       " 'JPM': {'J.P. Morgan'}}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /workspace/flair/app/app\n",
    "import json \n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from elasticsearch_dsl import Q\n",
    "import es\n",
    "\n",
    "es.init()\n",
    "\n",
    "domain = \"cnbc\"\n",
    "\n",
    "s = es.Page.search()\n",
    "q = Q('wildcard', from_url=f'*{domain}*') & Q(\"term\", http_status=200)\n",
    "\n",
    "# data = {\"text\": [], \"labels\": []}\n",
    "docs = []\n",
    "labels = []\n",
    "tickers = []\n",
    "\n",
    "ticker_name = {}\n",
    "ticker_count = {}\n",
    "keyword_count = {}\n",
    "\n",
    "def add(tickers, keywords):\n",
    "    for tk in tickers:\n",
    "        try:\n",
    "            ticker_count[tk] = \n",
    "        except:\n",
    "            ticker_count[tk] = \n",
    "        \n",
    "    \n",
    "\n",
    "for i, page in enumerate(s.filter(q).scan()):\n",
    "    if i > 0:\n",
    "        break\n",
    "\n",
    "    try:\n",
    "#         pprint(vars(page))\n",
    "#         meta = json.loads(page.article_metadata)\n",
    "#         pprint(meta)\n",
    "        parsed = json.loads(page.parsed)\n",
    "        pprint(parsed)\n",
    "        \n",
    "        _tickers = []\n",
    "        for e in parsed[\"tickers\"]:\n",
    "            for label in e[\"labels\"]:\n",
    "                name, ticker = label\n",
    "                try:\n",
    "                    ticker_name[ticker].add(name)\n",
    "                except:\n",
    "                    ticker_name[ticker] = set([name])\n",
    "                _tickers.append(ticker)\n",
    "        print(_tickers)\n",
    "        print(parsed[\"keywords\"])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "ticker_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Aerospace and defense industry',\n",
       " 'Airlines',\n",
       " 'Alibaba Group Holding Ltd',\n",
       " 'Alphabet Class A',\n",
       " 'Amazon.com Inc',\n",
       " 'Asia Economy',\n",
       " 'Asia News',\n",
       " 'Bank of America Corp',\n",
       " 'Banks',\n",
       " 'Boeing Co',\n",
       " 'Breaking News: Business',\n",
       " 'Breaking News: Markets',\n",
       " 'Breaking News: Technology',\n",
       " 'Business',\n",
       " 'CNBC: Stock Market & Business',\n",
       " 'Enterprise',\n",
       " 'Europe News',\n",
       " 'Finance',\n",
       " 'Germany',\n",
       " 'Housing',\n",
       " 'Investment strategy',\n",
       " 'LLC',\n",
       " 'Life',\n",
       " 'Markets',\n",
       " 'Microsoft Corp',\n",
       " 'NBCUniversal Media',\n",
       " 'News',\n",
       " 'Oracle Corp',\n",
       " 'Personal finance',\n",
       " 'Personal loans',\n",
       " 'Personal saving',\n",
       " 'Politics',\n",
       " 'Real estate',\n",
       " 'SAP AG',\n",
       " 'SVMK Inc',\n",
       " 'Slack Technologies Inc',\n",
       " 'Software',\n",
       " 'Southwest Airlines Co',\n",
       " 'Special Reports',\n",
       " 'Start-ups',\n",
       " 'Stock markets',\n",
       " 'Technology',\n",
       " 'Transportation',\n",
       " 'Travel',\n",
       " 'Wall Street',\n",
       " 'app',\n",
       " 'app store',\n",
       " 'appstore',\n",
       " 'business news',\n",
       " 'ios apps',\n",
       " 'ipad',\n",
       " 'iphone',\n",
       " 'ipod touch',\n",
       " 'itouch',\n",
       " 'itunes'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/LIAAD/yake\n",
      "  Cloning https://github.com/LIAAD/yake to /tmp/pip-req-build-9v41ayvi\n",
      "  Running command git clone -q https://github.com/LIAAD/yake /tmp/pip-req-build-9v41ayvi\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.7/site-packages (from yake==0.4.3) (0.8.7)\n",
      "Requirement already satisfied: click>=6.0 in /opt/conda/lib/python3.7/site-packages (from yake==0.4.3) (7.1.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from yake==0.4.3) (1.17.4)\n",
      "Requirement already satisfied: segtok in /opt/conda/lib/python3.7/site-packages (from yake==0.4.3) (1.5.7)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.7/site-packages (from yake==0.4.3) (2.4)\n",
      "Collecting jellyfish\n",
      "  Downloading jellyfish-0.8.2-cp37-cp37m-manylinux2014_x86_64.whl (90 kB)\n",
      "\u001b[K     |████████████████████████████████| 90 kB 1.2 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from segtok->yake==0.4.3) (2020.4.4)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx->yake==0.4.3) (4.4.1)\n",
      "Building wheels for collected packages: yake\n",
      "  Building wheel for yake (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for yake: filename=yake-0.4.3-py2.py3-none-any.whl size=67215 sha256=68f7fd17891fd6f875e2a860135af97c2999746d5ba1591e40e7d13128c04b49\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-j1qhthdw/wheels/52/79/f4/dae9309f60266aa3767a4381405002b6f2955fbcf038d804da\n",
      "Successfully built yake\n",
      "Installing collected packages: jellyfish, yake\n",
      "Successfully installed jellyfish-0.8.2 yake-0.4.3\n"
     ]
    }
   ],
   "source": [
    "# !conda install -c conda-forge -y spacy \n",
    "# !pip install -U spacy\n",
    "# !python -m spacy download zh_core_web_sm\n",
    "!pip install git+https://github.com/LIAAD/yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "没人会否认，平井一夫就是是索粉与索尼相互养成的偶像。\n",
    "\n",
    "成都魅力赏现场，索尼新一波创纪录的业绩预期每公布一个数字，在场的粉丝总会“Wow”一声。这声“Wow”在一个人出现时到达了最大声。并不难猜，这个人就是平井一夫。\n",
    "\n",
    "\n",
    "实际上，从这个月一日起，平井一夫就不再担任索尼的总裁兼 CEO。他的新职位是退居幕后的索尼董事长。尽管如此，不管他是何种身份，也无论是粉丝之夜还是魅力赏，哪怕就是场外的体验展台，只要平井一夫现身，现场立马就会响起日语的、英语的、中文的叫喊声，躁动和哭腔。\n",
    "\n",
    "整个场面并不比大牌明星见面会逊色。\n",
    "\n",
    "一个日本企业家为什么在中国会获得如此高的知名度和号召力？套用到目前流行的明星养成选秀，这似乎也不难理解。\n",
    "\n",
    "\n",
    "时间退回六年前，那时的索尼正处于风雨飘摇的末路时期。产品创新的乏力、资本市场的背弃，“破产”并非只是一句简单的调侃。\n",
    "\n",
    "2012 年 4 月 1 日，平井一夫正式出任索尼公司 CEO 兼总裁。这时，PS 业务出身的他对于游戏铁粉之外的大多数索尼粉丝而言，只是一个素人。\n",
    "\n",
    "他在任内推行的“One Sony 政策”和“中期计划”则是一场旷日持久的养成直播。\n",
    "\n",
    "尽管难说尽善尽美，但在他的治下，一方面，索尼各个业务开始了产品线的精简；另一方面，在平井一夫的关照下，索尼成立了一个名为为 Future Lab （未来实验室）的创意研发项目。这项目的主要目的是向公众公开某些产品的原型，通过收集公众的想法意见来规划产品的发展方向。简单地说，该项目旨在让索尼那些黑科技原型机更能符合公众需求。\n",
    "\n",
    "\n",
    "如此之下，索尼开始焕发新的生机。到平井一夫改革初见成效的 2015 财年，按通用会计准则计，索尼在这一财年共实现了 81057 亿日元（717.32 亿美元）的营收，营业利润为 2942 亿日元（约合 26.04 亿美元），净利润为 1478 亿日元（约合 13.08 亿美元），而此前一年，索尼净利润为-1260 亿日元。\n",
    "\n",
    "在平井一夫即将离任的消息前后，索尼发布了 2017 财年 Q3 财报：销售额增长 11.5%，为 26723 亿日元。营业利润大涨 279.8%，达 3508 亿日元。所有业务（作者注：包括移动业务）均实现盈利，索尼 2017 财年前三季度累计盈利已达 7127 亿日元。并且，索尼在财报中将本财年营业利润预期再次提高，至 7200 亿日元。不出意外，索尼的利润在 2017 财年将创下 72 年历史的新高。\n",
    "\n",
    "索粉们也从此放心——有平井一夫在，索尼说什么也不可能破产。\n",
    "\n",
    "一个企业家偶像就此养成。\n",
    "\n",
    "在索粉们心目中，平井一夫早已是超越一般 CEO 的存在。粉丝们一方面做着有关于他的表情包，调侃地追问“索尼今天破产了吗？”，另一方面，在他发布会上的“Wow”和“Kando（感动）”感召下，心甘情愿地献上膝盖和钱包，借着平井一夫（的表情包），粉丝和索尼保持着一种微妙的平衡。\n",
    "\n",
    "\n",
    "如果说以上都是索粉们的自发行为，那么这场成都举行的索粉之夜上，索尼官方安排的节目《川剧变脸》中，演员最后一刻将脸变成“姨夫的微笑”便大有“官方逼死同人”之感。\n",
    "\n",
    "看到这样严肃认真又带头恶搞的索尼，铁粉的钱包基本是保不住了。\n",
    "\n",
    "\n",
    "而一个会沙画的索粉在画出平井一夫怀抱熊猫的画面并呈现在大屏幕上，而“姨夫”在一旁魔性微笑，是这场索粉之夜最让人动容的时刻，因为没有人知道这是不是这个力挽索尼于破产边缘的男人亲自参加的最后一场索粉之夜了。\n",
    "\n",
    "对于索粉而言，他们一年之内最重要的节日是 Sony Expo\n",
    "\n",
    "当然，这个活动还有一个简洁好记又十分贴切的名字——索尼魅力赏。从 2014 年开始，到 2017 年，这个活动先后在上海、北京、广州举办。\n",
    "\n",
    "\n",
    "于索尼而言，魅力赏是索尼中国、索尼移动、索尼互娱、索尼音乐、索尼影视等索尼在华企业看家本领和顶级“黑科技”的大汇演，是对之前财年的完整总结，也是对崭新一年的美好展望。\n",
    "\n",
    "2018 年 4 月 12 日，成都接过上海、北京、广州的衣钵，成为这些索尼在华企业的最新汇演舞台。\n",
    "\n",
    "而对于索尼的中国铁杆粉丝们来说，魅力赏则意味着一场庆典。某种意义上，因为魅力赏开幕当晚的“索粉之夜”的存在，魅力赏又不止于节日。它成为了索粉们一个“绝对立场”被打开与志同道合的伙伴合作补完的过程，是一次索粉们对于人生方向的自我审视。\n",
    "\n",
    "\n",
    "索尼魅力赏，还是索粉们的一场朝圣。\n",
    "\n",
    "既然是朝圣，这就意味着其门槛是不低的。虽不至于“三步一跪、九步一叩”，但魅力赏，尤其是参加索粉之夜的几十个索粉们都是从全国成千上万的报名索粉中选出来的。说这群粉丝是精挑细选出来的坚定信仰者，并不为过。\n",
    "\n",
    "让索粉们扬眉吐气的魅力赏，从索尼的业绩猛涨开始\n",
    "\n",
    "从 2003 年 4 月已经成为专有名词的“索尼震撼（Sony Shock）”起，索尼的多项业务、乃至整个公司都陷入了低迷状态。\n",
    "\n",
    "哪怕经过斯金格可以称为“巨变”的改革，从 2008 财年到 2014 财年，除 2012 财年外，索尼每一个财年都在亏损，其背后的原因与索尼在和韩国竞争对手的争夺中，为了市场占有率和营收而放弃利润有关，当然也和大地震等天灾有关。而 2012 财年的盈利则是因为索尼在当年出售了大楼等资产。\n",
    "\n",
    "和如今日渐成为产业的“偶像饭圈”逻辑相似，当偶像的事业进入低谷，摇摆的粉丝会脱粉，坚定的铁粉会憋着一口气进行更大力度的应（花）援（钱）。\n",
    "\n",
    "几十年持续的高质量消费电子品产出，和最近十年因为产品之外的因素而被看衰，这种独特的经历让索尼积累一批数量庞大又舍得花钱的铁粉。嗯，无论是对于品牌的深刻理解还是购买力，那些新晋的国产手机品牌粉丝比起索粉，只能算是后辈。\n",
    "\n",
    "整个行业忠诚度和购买力能与索粉相对比的是苹果的果粉。当两者又有极大的不同，至少，憋着一口气的索粉总是勇于一边自黑、一边应援。这个圈子里的知名博主“今天 SONY 破产了吗 ”，看似戏谑的背后，其实质是一个一直用着索尼手机的粉丝。\n",
    "\n",
    "话又说回去，即使每天都在进行“破产没有？”的灵魂拷问，索粉们最乐于见到的依然是索尼恢复往日荣耀。\n",
    "\n",
    "足以让这些粉丝们扬眉吐气的是，索尼在本次魅力赏上带来了业绩和产品的好消息。\n",
    "\n",
    "索尼中国总裁高桥洋在活动的一开始就公布：\n",
    "\n",
    "集团全年有望创造 7200 亿日元营业利润的历史最佳业绩。此外，仅在 2017 年上半年，索尼在中国市场的业绩同比增长超过 30%。从由电视、数码影像、音频组成的消费电子业务，到 PlayStation 业务，再到索尼音乐等业务都可以说在各自领域取得了优势。\n",
    "这里再说几个数字。\n",
    "\n",
    "索尼电视在 2017 年 OLED 彩电市场的销售份额（金额）为 34%，排名第一、“65 英寸及以上市场份额（金额）排名第一”、“一万元人民币以上彩电市场份额（数量）排名第一”。\n",
    "\n",
    "索尼游戏业务正式回归中国三年，共计发行了 150 多款游戏作品。截止 2017 年 12 月 31 日，PS4 全球销量超过 7360 万台，游戏作品软件销量为 6 亿 4500 万张，PlayStation Plus 会员突破 3150 万人。PS VR 全球销量达到 200 万台，出现了超过 150 款精品 PS VR 游戏，全球 PS VR 游戏销量超过 1220 万张。\n",
    "\n",
    "\n",
    "世界上 TOP 6 的智能手机公司均是索尼影像传感器的客户。在介绍这一部分时，高桥洋还曝出了金句，“现在用索尼手机的人不多，但不用索尼传感器的手机很少”。索尼移动不姓索，算是官宣了（手动斜眼）。\n",
    "\n",
    "\n",
    "如果说，这些数字太抽象的话，一个更具象的变化在说明索尼的财政状况确实在不断向好：相比去前年魅力赏逼仄的场地，今年，整个成都可谓都被索尼承包。从春熙路到来福士广场，这些成都的商业地标都充满了索尼的印记。\n",
    "\n",
    "后记\n",
    "\n",
    "不出意外，一份创造索尼历史新高的财报将在一个月之后正式发布，而它的分量索尼和索粉都明白。\n",
    "\n",
    "厂家拿出优秀的产品，消费者实打实地花钱去购买。\n",
    "\n",
    "什么是“你只管认真，我们帮你赢”？\n",
    "\n",
    "这就是。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.zh import Chinese\n",
    "import yake\n",
    "\n",
    "nlp = Chinese()\n",
    "doc = nlp(text)\n",
    "t = \" \".join([tk.text for tk in doc])\n",
    "\n",
    "\n",
    "# text = \"蘋果 公司 正 考量 用 一 億元 買下 英國 的 新創 公司 ， 加上 一個 標點 。 \"\n",
    "\n",
    "extractor = yake.KeywordExtractor(lan=\"zh\", n=1)\n",
    "keywords = extractor.extract_keywords(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('亿日元', 0.0026444410756110803),\n",
       " ('索粉们', 0.002976195195791563),\n",
       " ('sony', 0.00671506498927978),\n",
       " ('wow', 0.00852361357827606),\n",
       " ('ceo', 0.009200188800847072),\n",
       " ('亿美元', 0.013942824262447678),\n",
       " ('另一方面', 0.0252324717162599),\n",
       " ('不出意外', 0.02604320100545872),\n",
       " ('如果说', 0.02604320100545872),\n",
       " ('市场份额', 0.02604320100545872),\n",
       " ('净利润', 0.02645257793868751),\n",
       " ('扬眉吐气', 0.02645257793868751),\n",
       " ('一口气', 0.02645257793868751),\n",
       " ('传感器', 0.02645257793868751),\n",
       " ('无论是', 0.026864608904568106),\n",
       " ('企业家', 0.026864608904568106),\n",
       " ('一方面', 0.026864608904568106),\n",
       " ('意味着', 0.026864608904568106),\n",
       " ('购买力', 0.026864608904568106),\n",
       " ('高桥洋', 0.026864608904568106)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.zh import Chinese\n",
    "\n",
    "nlp = Chinese()\n",
    "doc = nlp(u\"蘋果公司正考量用一億元買下英國的新創公司，加上一個標點。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[蘋果, 公司, 正, 考量, 用, 一, 億元, 買, 下, 英國, 的, 新創, 公司, ，, 加上, 一個, 標點, 。]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tk for tk in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Han NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[38;5;1m✘ No compatible model found for 'zh_core_web_sm' (spaCy v2.2.3).\u001b[0m\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# spacy\n",
    "\n",
    "# from spacy.lang.zh import Chinese\n",
    "\n",
    "# Chinese.Defaults.use_jieba = False\n",
    "# nlp = Chinese()\n",
    "\n",
    "# # Disable jieba through tokenizer config options\n",
    "# cfg = {\"use_jieba\": False}\n",
    "# nlp = Chinese(meta={\"tokenizer\": {\"config\": cfg}})\n",
    "\n",
    "# # Load with \"default\" model provided by pkuseg\n",
    "# cfg = {\"pkuseg_model\": \"default\", \"require_pkuseg\": True}\n",
    "# nlp = Chinese(meta={\"tokenizer\": {\"config\": cfg}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# abcnews (event clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of      publish_date                                      headline_text\n",
       "0        20030219  aba decides against community broadcasting lic...\n",
       "1        20030219     act fire witnesses must be aware of defamation\n",
       "2        20030219     a g calls for infrastructure protection summit\n",
       "3        20030219           air nz staff in aust strike for pay rise\n",
       "4        20030219      air nz strike to affect australian travellers\n",
       "..            ...                                                ...\n",
       "995      20030224                conference to focus on tuna fishery\n",
       "996      20030224                      council hosts farewell for mp\n",
       "997      20030224                council resists eba roster pressure\n",
       "998      20030224                   customs house restoration opened\n",
       "999      20030224              dam water levels still critically low\n",
       "\n",
       "[1000 rows x 2 columns]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/workspace/flair/data/abcnews-date-text.csv\")\n",
    "df = df.head(1000)\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentPoolEmbeddings, DocumentRNNEmbeddings\n",
    "\n",
    "document_embeddings = DocumentPoolEmbeddings([WordEmbeddings('glove')])\n",
    "\n",
    "X = []\n",
    "for i, d in enumerate(df[\"headline_text\"]):\n",
    "    sent = Sentence(d, use_tokenizer=True)\n",
    "    document_embeddings.embed(sent)\n",
    "    x = sent.get_embedding()\n",
    "    x = x.detach().numpy()\n",
    "    X.append(x)\n",
    "X = np.stack(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將doc vectors依照日期分成不同的組\n",
    "\n",
    "# df.groupby(by=\"publish_date\").agg([\"count\"])\n",
    "\n",
    "groups = df.groupby(by=\"publish_date\")\n",
    "groups.groups.keys()\n",
    "keys = [20030219, 20030220, 20030221, 20030222, 20030223, 20030224]\n",
    "\n",
    "\n",
    "# groups[20030219]\n",
    "# groups.get_group(20030219)\n",
    "d0 = groups.get_group(keys[0])\n",
    "d1 = groups.get_group(keys[1])\n",
    "d2 = groups.get_group(keys[2])\n",
    "\n",
    "# for i, d in enumerate(df[\"headline_text\"]):\n",
    "#     print(i, d)\n",
    "#     break\n",
    "\n",
    "X0 = X[d0.index[0]:d0.index[-1]+1]\n",
    "X1 = X[d1.index[0]:d1.index[-1]+1]\n",
    "# X2 = X[d2.index[0]:d2.index[-1]+1]\n",
    "# X0_1 = X[d0.index[0]:d1.index[-1]+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN, AffinityPropagation, MeanShift, AgglomerativeClustering\n",
    "\n",
    "clustering = KMeans(n_clusters=50, random_state=0).fit(X0)\n",
    "# clustering.predict(X1)\n",
    "# y = np.concatenate([c0.labels_, clustering.predict(X1)])\n",
    "# c1 = KMeans(n_clusters=45, random_state=0).fit(X1)\n",
    "\n",
    "# # z = np.zeros(df.shape[0], dtype=np.int32)\n",
    "# z = np.empty((df.shape[0],)) * np.nan\n",
    "# z[:c0.labels_.shape[0]] = c0.labels_\n",
    "# df[\"c0\"] = z\n",
    "\n",
    "# z = np.empty((df.shape[0],)) * np.nan\n",
    "# z[:c1.labels_.shape[0]] = c1.labels_\n",
    "# df[\"c1\"] = z\n",
    "# df.head(100)\n",
    "\n",
    "# _df = df[0:len(y)]\n",
    "# _df[\"y\"] = y\n",
    "# # _df.loc[_df[\"y\"] == 20, [\"headline_text\", \"y\"]]\n",
    "# _df.groupby(\"y\").agg([\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('t0-1', 2)\n",
      "('t0-1', 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{5: ['t0-1'], 6: ['t0-1'], 8: ['t0-1'], 7: ['t1-2'], 9: ['t1-2']}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"a\" : [4 ,5, 6]})\n",
    "# df.insert(loc=1, column=\"b\", value=[1,2])\n",
    "c = np.full((3,), np.nan).astype(np.uint8)\n",
    "c[1:3] = [1,2]\n",
    "df[\"c\"] = c\n",
    "df\n",
    "\n",
    "prev = [1,1,1,np.nan]\n",
    "cur = [1,2,3,4]\n",
    "\n",
    "cluster = {}\n",
    "# docs = pd.DataFrame({\"doc\" : [})\n",
    "df = pd.DataFrame({\n",
    "    \"prev\" : [\"t0-1\" ,\"t0-1\", \"t0-1\",\"NA\", \"NA\"], \n",
    "    \"cur\": [1,1,2,1,2]\n",
    "}, index = [5, 6, 7, 8, 9])\n",
    "\n",
    "\n",
    "# g = grouped.get_group(1)\n",
    "# list(grouped.groups)\n",
    "# grouped\n",
    "# print(grouped.get_group(2))\n",
    "\n",
    "\n",
    "# df = grouped.get_group(1)\n",
    "# print(df)\n",
    "# g = grouped.get_group(1).groupby('prev')\n",
    "\n",
    "# g.apply(lambda x: )\n",
    "# [c for c in g.count()]\n",
    "# g.count()\n",
    "\n",
    "# len(df)\n",
    "\n",
    "# grouped.get_group(2)\n",
    "# for g in grouped:\n",
    "#     print(g.)\n",
    "\n",
    "   \n",
    "    \n",
    "def set_cluster(df: pd.DataFrame, cur_cluster: int):\n",
    "    count = {}\n",
    "\n",
    "    for i, r in df.iterrows():\n",
    "        try:\n",
    "            count[r[\"prev\"]].append(i)\n",
    "        except KeyError:\n",
    "            count[r[\"prev\"]] = [i]\n",
    "\n",
    "    # 找出最大的prev_cluster\n",
    "    cmax = (None, 0)  # (prev_label, count)\n",
    "    total = 0 # number of prev items\n",
    "    for k, v in count.items():\n",
    "        if len(v) > cmax[1]:\n",
    "            cmax = (k, len(v))\n",
    "        if k != -1:\n",
    "            total += len(v)\n",
    "\n",
    "    print(cmax)\n",
    "\n",
    "    # 判斷該cluster是否為已建立event的延續\n",
    "    for i, r in df.iterrows():\n",
    "        c = cmax[0] if cmax[1] / total > 0.5 else f't1-{cur_cluster}'\n",
    "        try:\n",
    "            cluster[i].append(c)\n",
    "        except KeyError:\n",
    "            cluster[i] = [c]\n",
    "            \n",
    "    \n",
    "\n",
    "# k = 1\n",
    "# g = grouped.get_group(1)\n",
    "# df.iloc(1)\n",
    "# cluster[0].append(1)\n",
    "# cluster.values()\n",
    "\n",
    "grouped = df.groupby(\"cur\")\n",
    "for k, g in grouped:\n",
    "    set_cluster(g, cur_cluster=k)\n",
    "\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {\"a\": \"b\"}\n",
    "a[1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['community urged to help homeless youth', 'more women urged to become councillors', 'unions to ask members to support public protests']\n",
      "['gold coast to hear about bilby project', 'group to meet in north west wa over rock art', 'meeting to focus on broken hill water woes', 'mugabe to touch down in paris for summit', 'peace agreement may bring respite for venezuela', 'rice mill closures to put 300 out of work']\n",
      "['investigations underway into death toll of korean', 'investigation underway into elster creek spill']\n",
      "['cemeteries miss out on funds', 'de villiers to learn fate on march 5', 'dog mauls 18 month old toddler in nsw', 'england change three for wales match', 'griffiths under fire over project knock back', 'hanson should go back where she came from nsw mp', 'harrington raring to go after break', 'martin to lobby against losing nt seat in fed', 'more than 40 pc of young men drink alcohol at', 'mp raises hospital concerns in parliament', 'national gallery gets all clear after', 'oh brother your times up says ganguly senior', 'omodei to stay in politics', 'png nurses strike after colleague raped on way to', 'reading go third in first division', 'sa premier calls for action over river murray', 'stop changing the rules fans tell afl', 'taylor denies calling on waugh to quit', 'vic local councils welcome single polling day', 'vowles to retire at end of season', 'wine chief bounces back from sacking']\n",
      "['mp rejects ambulance levy claims']\n",
      "['australia to contribute 10 million in aid to iraq', 'businesses should prepare for terrorist attacks', 'funds allocated for domestic violence victims', 'funds allocated for youth at risk', 'funds announced for bridge work', 'iraq to pay for own rebuilding white house']\n",
      "['golf club feeling smoking ban impact', 'smoking bans hit tabcorp bottom line']\n",
      "['irish man arrested over omagh bombing', 'man arrested after central qld hijack attempt', 'man charged over cooma murder', 'man jailed over keno fraud', 'refshauge wins defamation court case', 'snowtown murder trial delayed']\n",
      "['fed opp to re introduce national insurance', 'regulator to inspect gm canola trials']\n",
      "['man fined after aboriginal tent embassy raid', 'nca defends aboriginal tent embassy raid', 'police defend aboriginal tent embassy raid']\n",
      "['death toll continues to climb in south korean subway', 'dying korean subway passengers phoned for help', 'korean subway fire 314 still missing', 'questions public anger grows after korean subway', 'search continues for victims in south korean subway', 'still no sign of missing fisherman', 'tasmanian scientists to search for east coast']\n",
      "['dargo fire threat expected to rise', 'moderate lift in wages growth', 'slow recovery predicted for aust economy']\n",
      "['onesteel to invest 80m in whyalla steelworks']\n",
      "['air nz staff in aust strike for pay rise', 'air nz strike to affect australian travellers', 'low demand forces air service cuts', 'nt govt boosts nurse number with overseas intake', 'qantas international crews to strike over pay', 'qantas war plan to cut 2500 jobs outrages unions', 'surge in new car sales in aust abs']\n",
      "['australia is locked into war timetable opp', 'blizzard buries united states in bills', 'digital tv will become commonplace summit', 'new zealand imposes visa entry for zimbabwe', 'nsw govt under fire for holding back vegetation', 'nsw opp defends claims of running race campaign', 'osullivan in world cross country doubt', 'qr not planning northern route sackings']\n",
      "['kelly disgusted at alleged bp ethanol scare', 'radioactive spill at wmcs olympic dam mine', 'worksafe probes potato harvester injuries']\n",
      "['testing shows dioxin above drinking water standards']\n",
      "['firefighters contain acid spill']\n",
      "['a g calls for infrastructure protection summit', 'bathhouse plans move ahead', 'plan for second skatepark']\n",
      "['israeli forces push into gaza strip', 'six palestinians killed in gaza incursion']\n",
      "['act fire witnesses must be aware of defamation', 'policewomen accusations feature at federal crime', 'public warned about phone scam']\n",
      "['dispute over at smithton vegetable processing plant', 'greens offer police station alternative', 'health minister backs organ and tissue storage', 'heavy metal deposits survey nearing end', 'irrigators vote over river management', 'massive drug crop discovered in western nsw', 'more water restrictions predicted for northern tas', 'no side effects for new whooping cough vaccine', 'program to monitor forest harvested areas', 'report highlights container terminal potential', 'uni to continue tree disease study', 'vff to buy stock feed pellets for fire affected']\n",
      "['inquest finds mans death accidental']\n",
      "['aba decides against community broadcasting licence', 'code of conduct toughens organ donation regulations', 'council chief executive fails to secure position', 'council moves to protect tas heritage garden', 'expressions of interest sought to build livestock', 'nato gives green light to defend turkey', 'opposition urged to help protect recherche bay', 'pagan says rule changes not necessary', 'patterson defends decision not to attend health', 'wildlife sanctuaries plan revealed']\n",
      "['probe launched into plane crash']\n",
      "['german court to give verdict on sept 11 accused', 'jury to consider verdict in murder case', 'pair to face court over ayr murder', 'teen to face court over drug charges']\n",
      "['antic delighted with record breaking barca', 'bryant leads lakers to double overtime win', 'last minute call hands alinghi big lead', 'williams says tight bowling key to warriors win']\n",
      "['injured rios pulls out of buenos aires open']\n",
      "['council welcomes ambulance levy decision', 'council welcomes insurance breakthrough']\n",
      "['funds to go to cadell upgrade', 'funds to help restore cossack']\n",
      "['nursing student intake down']\n",
      "['aussie qualifier stosur wastes four memphis match', 'calleri avenges final defeat to eliminate massu', 'dent downs philippoussis in tie break thriller']\n",
      "['big plan to boost paroo water supplies', 'call for ethanol blend fuel to go ahead', 'epa still trying to recover chemical clean up costs', 'kelly not surprised ethanol confidence low', 'meeting to consider tick clearance costs', 'plan to encourage farmers into plantation timber', 'public urged to check gas cylinders', 'states may be forced to label ethanol fuel', 'sugar industry plan to be revealed']\n",
      "['mayor warns landfill protesters']\n",
      "['commonwealth bank cuts fixed home loan rates', 'freedom records net profit for third successive', 'hacker gains access to eight million credit cards', 'record amount for gladstone ventures', 'resource stocks boost all ords', 'stations to get fixed home phone service']\n",
      "['nth koreans seek asylum at japanese embassy']\n",
      "['brigadier dismisses reports troops harassed in', 'bushfire victims urged to see centrelink', 'crean tells alp leadership critics to shut up', 'dems hold plebiscite over iraqi conflict', 'direct anger at govt not soldiers crean urges', 'govt is to blame for ethanols unpopularity opp', 'iraqs neighbours plead for continued un inspections', 'patterson no show displays govts arrogance crean', 'restraint order issued against anti discrimination', 'rfs rejects claim that act authorities spurned', 'rsl angry about reports of troop harassment', 'saudi arabians to stand trial over al qaeda', 'talk of asian nuclear arms race unhelpful downer']\n",
      "['nsw opp pledges 50m drought relief', 'rain eases wheatbelt water woes']\n",
      "['four injured in head on highway crash', 'girl injured in head on highway crash', 'police cracking down on driver safety']\n",
      "['aust addresses un security council over iraq', 'second resolution on iraq expected shortly un']\n",
      "['big hopes for launceston cycling championship', 'webb favourite for ladies masters']\n",
      "['barca take record as robson celebrates birthday in', 'carews freak goal leaves roma in ruins', 'hanson is grossly naive over nsw issues costa', 'orientation begins for uni students', 'patterson snubs health meeting to avoid lions den', 'shire offers assurances over finances', 'sterrey to steer sharks', 'swiss challengers looking to future', 'taipans placing future in publics hands', 'wales coach accuses players of belittling red', 'widnes abandon paul bid']\n",
      "['ambitious olsson wins triple jump']\n",
      "['councillor to contest wollongong as independent', 'most highly educated live in nsw wa', 'victorian scientists honoured at awards']\n",
      "['pienaar shines as ajax frustrate arsenal']\n",
      "['juvenile sex offenders unlikely to reoffend as']\n",
      "['gilchrist backs rest policy']\n",
      "['british combat troops arriving daily in kuwait', 'saudi arabia tells arabs war on iraq inevitable', 'us british aircraft attack sth iraq target']\n",
      "['man with knife hijacks light plane']\n",
      "['rabbit control program on trial', 'ricciuto undergoes surgery on injured ankle', 'safety review begins after bushwalker death', 'thousands remember 61st anniversary of darwin', 'warne hearing set for friday']\n"
     ]
    }
   ],
   "source": [
    "# 查看cluster內容\n",
    "clustering = KMeans(n_clusters=100, random_state=0).fit(X)\n",
    "\n",
    "# len(d0)\n",
    "# len(clustering.labels_)\n",
    "# d0['cluster'] = [i for i in range(198)]\n",
    "df[\"cluster\"] = clustering.labels_\n",
    "grouped = d0.groupby(\"cluster\")\n",
    "series = grouped.apply(lambda x: x[\"headline_text\"].tolist())\n",
    "# series.to_frame()\n",
    "for x in series:\n",
    "    print(x)\n",
    "\n",
    "# for i, x in grouped:\n",
    "#     x[\"headline_text\"]\n",
    "# def f(group):\n",
    "#     return pd.DataFrame({'original': group, 'demeaned': })\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f775b6baad0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23, 20, 18, 13, 13, 42, 26, 31, 39, 14,  5, 41, 18, 40, 32, 14, 36,\n",
       "       47, 26, 36,  5, 31, 32, 41,  3, 23, 34,  0, 23, 43, 23, 28, 28, 36,\n",
       "       11, 10, 36, 31,  3, 14, 36, 21,  3, 10,  3, 32, 23,  8, 17, 38, 34,\n",
       "        5,  5,  5, 29, 29, 25, 46, 38,  1,  6, 36, 21,  3,  1, 34, 41,  3,\n",
       "        3, 21, 21, 27, 22,  2,  2, 36,  5,  7, 21, 19, 25, 45, 15, 32, 10,\n",
       "       26, 13,  7,  7,  9,  7, 48,  3, 21, 33, 32,  1, 11,  3, 21,  0, 43,\n",
       "        3,  4,  1,  3, 23,  9, 14, 21, 14, 14, 37, 13, 35, 30,  3,  3, 12,\n",
       "       23, 41, 14, 23, 25, 23, 36, 41,  1, 44, 18, 32,  3, 38,  9, 20, 24,\n",
       "       21, 32, 20, 13, 13, 14, 10, 49, 15, 37,  3, 34,  7,  8, 21, 34, 36,\n",
       "       36, 49,  1, 36, 49,  3, 36, 47, 10, 39, 41, 19, 11,  6,  7, 32, 34,\n",
       "       41, 10,  3, 32, 13, 41, 41, 36, 10,  3, 25, 16, 49,  0, 21, 47, 21,\n",
       "        3, 43,  3, 41, 49, 40, 41, 23, 26,  3, 15], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c0 = KMeans(n_clusters=50, random_state=0).fit(X0)\n",
    "c0.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>c0</th>\n",
       "      <th>c1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>call for ethanol blend fuel to go ahead</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>health minister backs organ and tissue storage</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>last minute call hands alinghi big lead</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>more women urged to become councillors</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>omodei to stay in politics</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>rain eases wheatbelt water woes</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>uni to continue tree disease study</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>vowles to retire at end of season</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>brisbane sparkies head for 10 day strike</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>bungle leaves doctor waiting to practise</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>council general manager to step down</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      headline_text    c0   c1\n",
       "22          call for ethanol blend fuel to go ahead  18.0  2.0\n",
       "69   health minister backs organ and tissue storage   5.0  2.0\n",
       "85          last minute call hands alinghi big lead  22.0  2.0\n",
       "100          more women urged to become councillors  20.0  2.0\n",
       "117                      omodei to stay in politics  28.0  2.0\n",
       "145                 rain eases wheatbelt water woes  17.0  2.0\n",
       "184              uni to continue tree disease study  20.0  2.0\n",
       "189               vowles to retire at end of season  28.0  2.0\n",
       "215        brisbane sparkies head for 10 day strike   NaN  2.0\n",
       "219        bungle leaves doctor waiting to practise   NaN  2.0\n",
       "238            council general manager to step down   NaN  2.0"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = df.groupby(\"c0\")\n",
    "df.loc[df[\"c1\"] == 2, [\"headline_text\", \"c0\", \"c1\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 0, 0], dtype=int16)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3])\n",
    "z = np.zeros(5, dtype=np.int16)\n",
    "z[:a.shape[0]] = a\n",
    "# z\n",
    "# a.shape\n",
    "# a.shape[0]\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 82   0 120  91  91  65  26 109  21 115 212 148 234 225  30 107  73 105\n",
      "   3  75   0 153  49 148 213  36  40 132 114 260  15 229 229 159  33  90\n",
      "   9 109 135  93 159  47  23 168  42 219  76  76 156  24 170  84  84 120\n",
      " 254 254 205  70  24 193  45 159 151 154  15 212  12 115  79  36  48 169\n",
      " 214  90 161   9 120  52 136  62 205  60 185 130 168 100  91  52   8 203\n",
      "   8 166 176 121 244  76 120 171 152  30 132 260  73 150 135 115 183 203\n",
      "  87  25 213  32  39 190  10 207 240 135 122 132 181  41  19 106  19 217\n",
      " 126 226 259 120 128 154  24 203  80  18 235  16   6  91 234  82   6  81\n",
      " 145 129  61 178 223  47 237 111 206 118 119  55  56  81 100 137  64  90\n",
      " 155  12  62 171  45  34  49  16 165 152  41 219  28  41 122  22  15  70\n",
      " 106 204 124 132  25 105  16 260 139  29  26 215 227  58 186 201 141  14\n",
      " 257 100 162 114 176 170 175  79  96  82 124 234 157  13 148  96  93  29\n",
      "  64  10 141 256 117 183 120  27 252 101 201  67 251  40  46  19  46 200\n",
      "   0 131  81  97 114  76  41 118 160  51  77 189  92 112  95  95  17 200\n",
      " 227 206 141 100 145 178  28 250  38 134 101  60  41 154 103  13 150 222\n",
      " 100  50  99  67  98  71 259 133  79  33  76 154  47 116 202 245  15 165\n",
      "  26 211 112 112 147  32 136 135   4 120 161  47 218 168  56  42 241  61\n",
      "  26 252  12 251 205 205  72 115  18  14 166  34   8  52 236 223 222 106\n",
      " 106   8 189 179  15 141  79  29 196 188 211 180 152 164 252 228  49 103\n",
      " 126 117  15 135 115 100 144 190  28  71 115 115 141 105  75 235  38 157\n",
      " 131 134  63 211   0  57  16  24 164  16  16 121  31  70 136 135 129  18\n",
      " 211 164   6 157 116 170  72 184 213 126  61  27 144 233  17  61  20  15\n",
      " 153 159 159   4 256 256   4  74  66  55 195 175 120  27 242  24 106 185\n",
      "   0  55  62 116   5 178  98  34 165 163 183 183  85 155  38  21  59 183\n",
      " 183 177  26  12  33 115 212 181 193 136 171 128 178 106 173  42 185  76\n",
      " 176   5 222 215 202 202   2  48 162 212  56  54  54  66 137 226  59 159\n",
      " 213  78  57 246  61 109  12  10  98 158 146 146 177 191  11 199 199 128\n",
      " 185  12 148 180  75  25  22 251  71  74  71  28 133 234  68 217 142  27\n",
      " 180  35 115  19  84  80  80 130 117  76 201 253 258  51 254   5 127  39\n",
      "  95  46 118  27  12  84  12  33 135  47  13  32  58   7  89  65 100 193\n",
      " 130 130 169  81 118 165  63  15 249 148  44  52 133  87 120 161 238 240\n",
      " 147  12  79 100  43 114  61   5 217  30  43 220  30  23  52   8   8 118\n",
      " 236 236   8   8   8 106 100 141 110 244 120   2  30 167  73  73 141  19\n",
      "  42  26 103 216  22 159 178  23  39 121 207 102  42  29 122 106  62 217\n",
      " 217  50  71 154 186 195  21  63 112  63 245   8  57   6 112 219  15 131\n",
      "  98 120 150 185 219 208  17  95  95 107  95  30  37  33  59 102  61  78\n",
      " 110 176  52 228   4 125  54 136  54  12  37  50  76  49  49  25  18 219\n",
      " 230  94  94  94 224 226 148 113 183 183 183  85  19  59 193  80  76  77\n",
      " 123  27 123 150  29   0 215 182 135 108   5  79  55 260  54 139 220 137\n",
      "  85  41  42 248   9  23 118 113  11  29  19  86  99  64 192 176 153  35\n",
      "   4 172 227  54 112 123  43  83 108 160  67  55 153  32 127   1 175 240\n",
      "  21 221 221 205  68 119  61   7  44  69 201 183   2  11   4  85  67 175\n",
      " 247  27 239 151   3 194  41  82  50  43   6 199  71 168 168   2  71 154\n",
      " 172   8  22  22 187 226 162   2  27 107 160  81 249 252   9  83 241  89\n",
      "  52  79 168 230 109  52 154 225  86 174  75 115  28  88  76 158  54 183\n",
      "  52 115 197  54 252  54 205  59 213 154 215 144  61  14   8 162  50 173\n",
      "  54 134  77   5 130 112 108 155  98 184  21  63  88  32   1 138   1  35\n",
      " 167 178  40  34  70  92  29 227 210 154 234  12 112  22 194 129 154 101\n",
      " 242   7   1 225  33 257  29 154 209  29 154 121  82 148  97  70 249  70\n",
      " 109  26 148  45 255   7  44 194   3 104   8  52   8   8 158 189 122 153\n",
      "   0  23  89 116  78 113 112 257 115 190  26  30  64  19  12  11 166  83\n",
      " 188  24 112 148 107  53  96 252  21  18 243 125 125 107 220  99 107 140\n",
      " 140  20  17 181  23  43 125  29 134   1 115 183 228 155 155 155 215 230\n",
      "  17  61  29 201  21 218  90  46   8   5 111 149  25  31  97 257  69  71\n",
      " 143 108  13  41 157 108  56 120  50 213  53  61   6 257 152  80  30 232\n",
      " 231 219  59 157  16  50 260 126 198 204]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>former star koen licks cats</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>cat lovers march for romes sacred strays</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>cats claw dogs pies swoop on blues</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>fake crocs scare off flamingos</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>tourists cry foul over spilt milk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                headline_text  y\n",
       "735               former star koen licks cats  1\n",
       "842  cat lovers march for romes sacred strays  1\n",
       "844        cats claw dogs pies swoop on blues  1\n",
       "866            fake crocs scare off flamingos  1\n",
       "945         tourists cry foul over spilt milk  1"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN, AffinityPropagation, MeanShift, AgglomerativeClustering\n",
    "\n",
    "# clustering = KMeans(n_clusters=100, random_state=0).fit(X)\n",
    "# clustering = DBSCAN(eps=0.11, min_samples=1, metric=\"cosine\").fit(X)\n",
    "# clustering = AffinityPropagation(max_iter=2000, convergence_iter=15,).fit(X)\n",
    "# clustering = MeanShift(max_iter=3000).fit(X)\n",
    "# clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=3).fit(X)\n",
    "\n",
    "print(clustering.labels_)\n",
    "df[\"y\"] = list(clustering.labels_)\n",
    "\n",
    "# print(kmeans.predict([[0, 0], [12, 3]]))\n",
    "# print(kmeans.cluster_centers_)\n",
    "# df[\"label\"] = kmeans.labels_\n",
    "# df[df[\"label\"] == 0]\n",
    "\n",
    "# len(df)\n",
    "df.loc[df[\"y\"] == 1, [\"headline_text\", \"y\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 0, 0], dtype=int16)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"a\" : [1,2,3,4,5],\"b\" : [6, 7, 8, 9,10]})\n",
    "# df[\"d\"] = None\n",
    "# df.loc[0, \"d\"] = 1\n",
    "# df[\"a\"].tolist()\n",
    "# df.loc[0:2, \"c\"] = [0,1]\n",
    "# df[\"c\"] = None\n",
    "# df.loc[0:2, \"c\"] = [0,1,2]\n",
    "df[\"c\"] = np.zeros(5).astype(np.int16)\n",
    "df\n",
    "# df[\"c\"].apply(pd.to_numeric)\n",
    "a = np.zeros(5).astype(np.int16)\n",
    "a[0:3] = np.array([1,2,3])\n",
    "a\n",
    "\n",
    "# groups = df.groupby(by=\"publish_date\")\n",
    "# groups.iloc['20030219']\n",
    "# .agg([\"count\"])\n",
    "# for name, group in groups:\n",
    "#     print(name)\n",
    "#     for i, row in group.iterrows():\n",
    "#         print(row['headline_text'])\n",
    "#         col = row['column']\n",
    "#         column_type = row['column_type']\n",
    "#         is_null = 'NOT NULL' if row['is_null'] == 'NO' else ''\n",
    "#         print('\\t{} {} {},'.format(col, column_type, is_null))\n",
    "\n",
    "# gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'contributors': None,\n",
      " 'coordinates': None,\n",
      " 'created_at': 'Tue Oct 14 13:39:49 +0000 2014',\n",
      " 'entities': {'hashtags': [],\n",
      "              'symbols': [{'indices': [0, 4], 'text': 'GDP'},\n",
      "                          {'indices': [42, 47], 'text': 'TSLA'},\n",
      "                          {'indices': [48, 53], 'text': 'TWTR'},\n",
      "                          {'indices': [54, 56], 'text': 'C'},\n",
      "                          {'indices': [57, 61], 'text': 'GDP'},\n",
      "                          {'indices': [62, 67], 'text': 'AMZN'},\n",
      "                          {'indices': [68, 73], 'text': 'DRYS'},\n",
      "                          {'indices': [74, 77], 'text': 'VZ'}],\n",
      "              'trends': [],\n",
      "              'urls': [{'display_url': 'twitter.com/CBOE/status/52…',\n",
      "                        'expanded_url': 'https://twitter.com/CBOE/status/522018942626594816',\n",
      "                        'indices': [81, 104],\n",
      "                        'url': 'https://t.co/7Q6GX3gUuS'},\n",
      "                       {'display_url': 'quantpost.com/list/?symbol=G…',\n",
      "                        'expanded_url': 'https://quantpost.com/list/?symbol=GDP',\n",
      "                        'indices': [117, 140],\n",
      "                        'url': 'https://t.co/3mM5DHJBo5'}],\n",
      "              'user_mentions': [{'id': 16827489,\n",
      "                                 'id_str': '16827489',\n",
      "                                 'indices': [28, 33],\n",
      "                                 'name': 'CBOE',\n",
      "                                 'screen_name': 'CBOE'}]},\n",
      " 'favorite_count': 0,\n",
      " 'favorited': False,\n",
      " 'filter_level': 'medium',\n",
      " 'geo': None,\n",
      " 'id': 522018944454914050,\n",
      " 'id_str': '522018944454914050',\n",
      " 'in_reply_to_screen_name': None,\n",
      " 'in_reply_to_status_id': None,\n",
      " 'in_reply_to_status_id_str': None,\n",
      " 'in_reply_to_user_id': None,\n",
      " 'in_reply_to_user_id_str': None,\n",
      " 'lang': 'en',\n",
      " 'place': None,\n",
      " 'possibly_sensitive': False,\n",
      " 'retweet_count': 0,\n",
      " 'retweeted': False,\n",
      " 'source': '<a href=\"https://quantpost.com\" rel=\"nofollow\">Quantpost '\n",
      "           'Herald</a>',\n",
      " 'text': '$GDP News: \"Actives on open @CBOE: $ AAPL $TSLA $TWTR $C $GDP $AMZN '\n",
      "         '$DRYS $VZ …\" https://t.co/7Q6GX3gUuS Board view: '\n",
      "         'https://t.co/3mM5DHJBo5',\n",
      " 'timestamp_ms': '1413293989664',\n",
      " 'truncated': False,\n",
      " 'user': {'contributors_enabled': False,\n",
      "          'created_at': 'Tue Oct 01 16:13:12 +0000 2013',\n",
      "          'default_profile': False,\n",
      "          'default_profile_image': False,\n",
      "          'description': 'Option Traders: A real-time machine and human '\n",
      "                         'monitor of relevant Basic Materials Sector news and '\n",
      "                         'market events. See also '\n",
      "                         '@quantpost,@qp_basic,@qp_financial,...',\n",
      "          'favourites_count': 0,\n",
      "          'follow_request_sent': None,\n",
      "          'followers_count': 109,\n",
      "          'following': None,\n",
      "          'friends_count': 186,\n",
      "          'geo_enabled': False,\n",
      "          'id': 1923902360,\n",
      "          'id_str': '1923902360',\n",
      "          'is_translator': False,\n",
      "          'lang': 'en',\n",
      "          'listed_count': 5,\n",
      "          'location': '',\n",
      "          'name': 'Quantpost Basic Mats',\n",
      "          'notifications': None,\n",
      "          'profile_background_color': 'FFFFFF',\n",
      "          'profile_background_image_url': 'http://pbs.twimg.com/profile_background_images/378800000086180953/8400c43a4d4f8926bc84ddb373b8a88c.png',\n",
      "          'profile_background_image_url_https': 'https://pbs.twimg.com/profile_background_images/378800000086180953/8400c43a4d4f8926bc84ddb373b8a88c.png',\n",
      "          'profile_background_tile': False,\n",
      "          'profile_banner_url': 'https://pbs.twimg.com/profile_banners/1923902360/1411214979',\n",
      "          'profile_image_url': 'http://pbs.twimg.com/profile_images/513298893434417153/dy9NrfhO_normal.png',\n",
      "          'profile_image_url_https': 'https://pbs.twimg.com/profile_images/513298893434417153/dy9NrfhO_normal.png',\n",
      "          'profile_link_color': '009999',\n",
      "          'profile_sidebar_border_color': '000000',\n",
      "          'profile_sidebar_fill_color': 'DDEEF6',\n",
      "          'profile_text_color': '333333',\n",
      "          'profile_use_background_image': False,\n",
      "          'protected': False,\n",
      "          'screen_name': 'QP_Basic',\n",
      "          'statuses_count': 4916,\n",
      "          'time_zone': 'Mountain Time (US & Canada)',\n",
      "          'url': 'http://quantpost.com',\n",
      "          'utc_offset': -21600,\n",
      "          'verified': False}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import os \n",
    "import json \n",
    "directory = \"/workspace/stocknet-dataset/tweet/raw\"\n",
    "\n",
    "for ticker in os.listdir(directory):\n",
    "    p = os.path.join(directory, ticker)\n",
    "    if os.path.isfile(p):\n",
    "        continue\n",
    "    for fn in os.listdir(p):\n",
    "        with open(os.path.join(directory, ticker, fn)) as f:\n",
    "            for i, l in enumerate(f.readlines()):\n",
    "                print(i)\n",
    "                j = json.loads(l)\n",
    "#                 print(j[\"text\"])\n",
    "                pprint(j)\n",
    "#                 print(l)\n",
    "        break\n",
    "    break\n",
    "\n",
    "# [f for f in listdir(p)]\n",
    "# for root, dirnames, filenames in os.walk(d):\n",
    "#     for fileanme in filenames:\n",
    "#         p = os.path.join(root, dirname, filename)\n",
    "#         print(p)\n",
    "#         with open(p) as f:\n",
    "#             pass\n",
    "#         break\n",
    "        \n",
    "#     for filename in filenames:\n",
    "#         if \".DS_Store\" in filename:\n",
    "#             continue\n",
    "#         path = os.path.join(root, filename)\n",
    "#         print(root, filename)\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yields on CDs Fell in the Latest Week</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Morning Brief: White House Seeks to Limit ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Banking Bill Negotiators Set Compromise --- Pl...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Manager's Journal: Sniffing Out Drug Abusers I...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Currency Trading: Dollar Remains in Tight Rang...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stocks Fall Again; BofA, Alcoa Slide</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bank of Montreal, Royal Bank Profits Rose in 2...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Battle Over Medical Costs Isn't Over</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sequenom to Buy Gemini Genomics In Stock Accord</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>U.S. Dollar Falls Against Most Currencies; Dec...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Dow Falls 45.95, Late GM Surge Stanches Losses</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             headline  label\n",
       "0               Yields on CDs Fell in the Latest Week      0\n",
       "1   The Morning Brief: White House Seeks to Limit ...      1\n",
       "2   Banking Bill Negotiators Set Compromise --- Pl...      2\n",
       "3   Manager's Journal: Sniffing Out Drug Abusers I...      3\n",
       "4   Currency Trading: Dollar Remains in Tight Rang...      4\n",
       "5                Stocks Fall Again; BofA, Alcoa Slide      5\n",
       "6   Bank of Montreal, Royal Bank Profits Rose in 2...      6\n",
       "7                Battle Over Medical Costs Isn't Over      7\n",
       "8     Sequenom to Buy Gemini Genomics In Stock Accord      8\n",
       "9   U.S. Dollar Falls Against Most Currencies; Dec...      9\n",
       "10     Dow Falls 45.95, Late GM Surge Stanches Losses     10"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "df = pd.read_csv(\"/workspace/flair/data/Full-Economic-News-DFE-839861.csv\", encoding='ISO-8859-1')\n",
    "# documents = df.loc[:100, \"text\"].tolist()\n",
    "df = df.loc[:10, [\"headline\"]]\n",
    "df['label'] = [i for i in range(0,11)]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaa']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# documents\n",
    "df = pd.DataFrame({\"d\" : [\"aaa\" ,\"bbb\", \"ccc\"]})\n",
    "df['label'] = [1,1,0]\n",
    "df[df.label == 1]\n",
    "\n",
    "[x for x in df[\"d\"].head(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-04 15:40:02,350 loading file /root/.flair/models/en-ner-conll03-v0.4.pt\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# make a sentence\n",
    "sentence = Sentence('I love Berlin .')\n",
    "\n",
    "# load the NER tagger\n",
    "tagger = SequenceTagger.load('ner')\n",
    "\n",
    "# run NER over sentence\n",
    "tagger.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "X = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]])\n",
    "# X.shape\n",
    "# kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "# print(kmeans.labels_)\n",
    "# print(kmeans.predict([[0, 0], [12, 3]]))\n",
    "# print(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>positivity</th>\n",
       "      <th>positivity:confidence</th>\n",
       "      <th>relevance</th>\n",
       "      <th>relevance:confidence</th>\n",
       "      <th>articleid</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>positivity_gold</th>\n",
       "      <th>relevance_gold</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>842613460</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/4/15 23:15</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6783</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>wsj_905654974</td>\n",
       "      <td>11/23/11</td>\n",
       "      <td>Stocks Fall Again; BofA, Alcoa Slide</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stocks declined, as investors weighed slower-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>842613511</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/5/15 7:26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>1.0</td>\n",
       "      <td>wsj_905646207</td>\n",
       "      <td>11/23/11</td>\n",
       "      <td>Indian Rupee Falls Again</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MUMBAI - The Indian rupee fell against the U.S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "5   842613460    False   finalized                   3     12/4/15 23:15   \n",
       "56  842613511    False   finalized                   3      12/5/15 7:26   \n",
       "\n",
       "    positivity  positivity:confidence relevance  relevance:confidence  \\\n",
       "5          3.0                 0.6783       yes                   1.0   \n",
       "56         NaN                    NaN        no                   1.0   \n",
       "\n",
       "        articleid      date                              headline  \\\n",
       "5   wsj_905654974  11/23/11  Stocks Fall Again; BofA, Alcoa Slide   \n",
       "56  wsj_905646207  11/23/11              Indian Rupee Falls Again   \n",
       "\n",
       "    positivity_gold  relevance_gold  \\\n",
       "5               NaN             NaN   \n",
       "56              NaN             NaN   \n",
       "\n",
       "                                                 text  \n",
       "5   Stocks declined, as investors weighed slower-t...  \n",
       "56  MUMBAI - The Indian rupee fell against the U.S...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/workspace/flair/data/Full-Economic-News-DFE-839861.csv\", encoding='ISO-8859-1')\n",
    "df = df.head(100)\n",
    "# [d for d in df[\"headline\"]]\n",
    "# df.columns\n",
    "g = df.groupby([\"date\"])\n",
    "g.get_group('11/23/11')\n",
    "# s = g.size()\n",
    "# s[s > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/workspace/flair/data/abcnews-date-text.csv\")\n",
    "df = df.head(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "publish_date\n",
       "20030219    198\n",
       "20030220    250\n",
       "20030221    250\n",
       "20030222    126\n",
       "20030223    136\n",
       "20030224    250\n",
       "20030225    250\n",
       "20030226    250\n",
       "20030227    221\n",
       "20030228     69\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = df.groupby([\"publish_date\"])\n",
    "g.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Currency Trading: Dollar Remains in Tight Rang...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>U.S. Dollar Falls Against Most Currencies; Dec...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Dollar Declines as Players Take Profits From R...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Bond Prices Tumble as Dollar's Plunge Prompts ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Tuesday's markets: Prices of bonds, stocks dro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Retailers Stock Up on Caution</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Some Stocks Had Big Fourth-Quarter Gains Despi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Bond Prices Are Little Changed In Slow Trading...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Futures Markets: Treasury Bond Contracts End N...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>AMR, Southwest Profits Surge Even as Fuel Pric...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Use of stock-options contracts climbs as inves...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Asian Shares Mixed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Treasurys Prices Continue to Climb on Weakness...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Stocks Maintain Recent Gains, Giving Investors...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             headline  y\n",
       "4   Currency Trading: Dollar Remains in Tight Rang...  1\n",
       "9   U.S. Dollar Falls Against Most Currencies; Dec...  1\n",
       "17  Dollar Declines as Players Take Profits From R...  1\n",
       "26  Bond Prices Tumble as Dollar's Plunge Prompts ...  1\n",
       "35  Tuesday's markets: Prices of bonds, stocks dro...  1\n",
       "47                      Retailers Stock Up on Caution  1\n",
       "48  Some Stocks Had Big Fourth-Quarter Gains Despi...  1\n",
       "52  Bond Prices Are Little Changed In Slow Trading...  1\n",
       "60  Futures Markets: Treasury Bond Contracts End N...  1\n",
       "62  AMR, Southwest Profits Surge Even as Fuel Pric...  1\n",
       "80  Use of stock-options contracts climbs as inves...  1\n",
       "82                                 Asian Shares Mixed  1\n",
       "87  Treasurys Prices Continue to Climb on Weakness...  1\n",
       "91  Stocks Maintain Recent Gains, Giving Investors...  1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentPoolEmbeddings, DocumentRNNEmbeddings\n",
    "\n",
    "df = pd.read_csv(\"/workspace/flair/data/Full-Economic-News-DFE-839861.csv\", encoding='ISO-8859-1')\n",
    "df = df.head(100)\n",
    "\n",
    "document_embeddings = DocumentPoolEmbeddings([WordEmbeddings('glove')])\n",
    "\n",
    "X = []\n",
    "for d in df[\"headline\"]:\n",
    "    sent = Sentence(d, use_tokenizer=True)\n",
    "    document_embeddings.embed(sent)\n",
    "    x = sent.get_embedding()\n",
    "    x = x.detach().numpy()\n",
    "#     print(x)\n",
    "    X.append(x)\n",
    "\n",
    "X = np.stack(X)\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(X)\n",
    "# print(len(kmeans.labels_))\n",
    "df[\"y\"] = list(kmeans.labels_)\n",
    "# print(kmeans.predict([[0, 0], [12, 3]]))\n",
    "# print(kmeans.cluster_centers_)\n",
    "# df[\"label\"] = kmeans.labels_\n",
    "# df[df[\"label\"] == 0]\n",
    "\n",
    "# len(df)\n",
    "df.loc[df[\"y\"] == 1, [\"headline\", \"y\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Currency Trading: Dollar Remains in Tight Rang...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>U.S. Dollar Falls Against Most Currencies; Dec...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Dollar Declines as Players Take Profits From R...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Bond Prices Tumble as Dollar's Plunge Prompts ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Tuesday's markets: Prices of bonds, stocks dro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Retailers Stock Up on Caution</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Some Stocks Had Big Fourth-Quarter Gains Despi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Bond Prices Are Little Changed In Slow Trading...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Futures Markets: Treasury Bond Contracts End N...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>AMR, Southwest Profits Surge Even as Fuel Pric...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Use of stock-options contracts climbs as inves...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Asian Shares Mixed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Treasurys Prices Continue to Climb on Weakness...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Stocks Maintain Recent Gains, Giving Investors...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             headline  y\n",
       "4   Currency Trading: Dollar Remains in Tight Rang...  1\n",
       "9   U.S. Dollar Falls Against Most Currencies; Dec...  1\n",
       "17  Dollar Declines as Players Take Profits From R...  1\n",
       "26  Bond Prices Tumble as Dollar's Plunge Prompts ...  1\n",
       "35  Tuesday's markets: Prices of bonds, stocks dro...  1\n",
       "47                      Retailers Stock Up on Caution  1\n",
       "48  Some Stocks Had Big Fourth-Quarter Gains Despi...  1\n",
       "52  Bond Prices Are Little Changed In Slow Trading...  1\n",
       "60  Futures Markets: Treasury Bond Contracts End N...  1\n",
       "62  AMR, Southwest Profits Surge Even as Fuel Pric...  1\n",
       "80  Use of stock-options contracts climbs as inves...  1\n",
       "82                                 Asian Shares Mixed  1\n",
       "87  Treasurys Prices Continue to Climb on Weakness...  1\n",
       "91  Stocks Maintain Recent Gains, Giving Investors...  1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.stack(X)\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(X)\n",
    "# print(len(kmeans.labels_))\n",
    "df[\"y\"] = list(kmeans.labels_)\n",
    "# print(kmeans.predict([[0, 0], [12, 3]]))\n",
    "# print(kmeans.cluster_centers_)\n",
    "# df[\"label\"] = kmeans.labels_\n",
    "# df[df[\"label\"] == 0]\n",
    "\n",
    "# len(df)\n",
    "df.loc[df[\"y\"] == 1, [\"headline\", \"y\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 9, 1, 0, 2, 8, 3, 5, 6, 2, 4]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from flair.data import Sentence\n",
    "from flair.embeddings import Sentence, WordEmbeddings, FlairEmbeddings, DocumentPoolEmbeddings, DocumentRNNEmbeddings\n",
    "\n",
    "sentence = Sentence('this is one. this is two. this is three.', use_tokenizer=True)\n",
    "# sentence.tokens\n",
    "# now check out the embedded tokens.\n",
    "# for token in sentence:\n",
    "#     print(token)\n",
    "#     print(token.embedding)\n",
    "\n",
    "# document_embeddings = DocumentPoolEmbeddings([WordEmbeddings('glove'),\n",
    "#                                               FlairEmbeddings('news-backward'),\n",
    "#                                               FlairEmbeddings('news-forward')])\n",
    "document_embeddings = DocumentPoolEmbeddings([WordEmbeddings('glove')])\n",
    "document_embeddings.embed(sentence)\n",
    "# print(sentence.get_embedding())\n",
    "\n",
    "# from flair.embeddings import WordEmbeddings, DocumentRNNEmbeddings\n",
    "\n",
    "# glove_embedding = WordEmbeddings('glove')\n",
    "# document_embeddings = DocumentRNNEmbeddings([glove_embedding])\n",
    "# sentence = Sentence('The grass is green . And the sky is blue .')\n",
    "# document_embeddings.embed(sentence)\n",
    "# print(sentence.get_embedding())\n",
    "\n",
    "# document_lstm_embeddings = DocumentRNNEmbeddings([WordEmbeddings('glove')], rnn_type='LSTM')\n",
    "# document_embeddings.embed(sentence)\n",
    "# print(sentence.get_embedding())\n",
    "\n",
    "x = sentence.get_embedding()\n",
    "x.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "\n",
    "# sent = Sentence('The grass is green.', use_tokenizer=True)\n",
    "sent = Sentence('The grass is green.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-04 15:46:44,343 Reading data from /workspace/flair/data/@me_dataset\n",
      "2020-07-04 15:46:44,352 Train: /workspace/flair/data/@me_dataset/train.txt\n",
      "2020-07-04 15:46:44,359 Dev: /workspace/flair/data/@me_dataset/dev.txt\n",
      "2020-07-04 15:46:44,363 Test: /workspace/flair/data/@me_dataset/test.txt\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import ClassificationCorpus\n",
    "\n",
    "corpus: Corpus = ClassificationCorpus('/workspace/flair/data/@me_dataset',\n",
    "                                      test_file='test.txt',\n",
    "                                      dev_file='dev.txt',\n",
    "                                      train_file='train.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-04 15:51:44,909 Reading data from /workspace/flair/data/@me_dataset\n",
      "2020-07-04 15:51:44,918 Train: /workspace/flair/data/@me_dataset/train.txt\n",
      "2020-07-04 15:51:44,933 Dev: /workspace/flair/data/@me_dataset/dev.txt\n",
      "2020-07-04 15:51:44,951 Test: /workspace/flair/data/@me_dataset/test.txt\n",
      "2020-07-04 15:51:45,040 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-04 15:51:46,167 [b'aaa', b'bbb']\n",
      "2020-07-04 15:51:46,227 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-04 15:51:46,242 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=100, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512, batch_first=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): BCEWithLogitsLoss()\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2020-07-04 15:51:46,247 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-04 15:51:46,259 Corpus: \"Corpus: 1 train + 1 dev + 1 test sentences\"\n",
      "2020-07-04 15:51:46,276 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-04 15:51:46,290 Parameters:\n",
      "2020-07-04 15:51:46,323  - learning_rate: \"0.1\"\n",
      "2020-07-04 15:51:46,326  - mini_batch_size: \"32\"\n",
      "2020-07-04 15:51:46,328  - patience: \"5\"\n",
      "2020-07-04 15:51:46,352  - anneal_factor: \"0.5\"\n",
      "2020-07-04 15:51:46,355  - max_epochs: \"100\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-04 15:51:46,377  - shuffle: \"True\"\n",
      "2020-07-04 15:51:46,407  - train_with_dev: \"False\"\n",
      "2020-07-04 15:51:46,414  - batch_growth_annealing: \"False\"\n",
      "2020-07-04 15:51:46,423 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-04 15:51:46,443 Model training base path: \"resources/taggers/test\"\n",
      "2020-07-04 15:51:46,458 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-04 15:51:46,464 Device: cpu\n",
      "2020-07-04 15:51:46,532 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-04 15:51:46,548 Embeddings storage mode: cpu\n",
      "2020-07-04 15:51:46,601 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-04 15:51:47,692 epoch 1 - iter 1/1 - loss 0.83057070 - samples/sec: 62.67\n",
      "2020-07-04 15:51:49,305 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-04 15:51:49,324 EPOCH 1 done: loss 0.8306 - lr 0.1000000\n",
      "2020-07-04 15:51:50,936 DEV : loss 0.12883278727531433 - score 1.0\n",
      "2020-07-04 15:51:50,954 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-07-04 15:52:08,828 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-04 15:52:10,478 epoch 2 - iter 1/1 - loss 0.15428255 - samples/sec: 42.06\n",
      "2020-07-04 15:52:11,965 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-04 15:52:11,979 EPOCH 2 done: loss 0.1543 - lr 0.1000000\n",
      "2020-07-04 15:52:13,633 DEV : loss 0.06831537932157516 - score 1.0\n",
      "2020-07-04 15:52:13,716 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-07-04 15:52:17,217 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-04 15:52:17,259 Exiting from training early.\n",
      "2020-07-04 15:52:17,284 Saving model ...\n",
      "2020-07-04 15:52:39,728 Done.\n",
      "2020-07-04 15:52:39,759 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-04 15:52:39,763 Testing using best model ...\n",
      "2020-07-04 15:52:39,778 loading file resources/taggers/test/best-model.pt\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-3fad0c61d102>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m               \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m               \u001b[0manneal_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m               patience=5)\n\u001b[0m",
      "\u001b[0;32m/workspace/flair/flair/trainers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, base_path, learning_rate, mini_batch_size, mini_batch_chunk_size, max_epochs, scheduler, anneal_factor, patience, initial_extra_patience, min_learning_rate, train_with_dev, monitor_train, monitor_test, embeddings_storage_mode, checkpoint, save_final_model, anneal_with_restarts, anneal_with_prestarts, batch_growth_annealing, shuffle, param_selection_mode, num_workers, sampler, use_amp, amp_opt_level, eval_on_train_fraction, eval_on_train_shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;31m# test best model if test data is present\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0mfinal_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_chunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfinal_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/flair/flair/trainers/trainer.py\u001b[0m in \u001b[0;36mfinal_test\u001b[0;34m(self, base_path, eval_mini_batch_size, num_workers)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"best-model.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"best-model.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m         test_results, test_loss = self.model.evaluate(\n",
      "\u001b[0;32m/workspace/flair/flair/nn.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, model)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;31m# see https://github.com/zalandoresearch/flair/issues/351\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_big_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_model_with_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "from flair.data import segtok_tokenizer, Sentence\n",
    "from flair.datasets import ClassificationDataset\n",
    "from flair.datasets import ClassificationCorpus\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentPoolEmbeddings, DocumentRNNEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "# ds = ClassificationDataset(\n",
    "#     path_to_file='/workspace/flair/data/news/train.txt',\n",
    "#     in_memory=True,\n",
    "# )\n",
    "# ds[0]\n",
    "\n",
    "# corpus = ClassificationCorpus(data_folder='/workspace/flair/data/news', tokenizer=segtok_tokenizer, in_memory=True)\n",
    "# corpus.train[0]\n",
    "corpus: Corpus = ClassificationCorpus('/workspace/flair/data/@me_dataset',\n",
    "                                      test_file='test.txt',\n",
    "                                      dev_file='dev.txt',\n",
    "                                      train_file='train.txt',\n",
    "                                     tokenizer=segtok_tokenizer)\n",
    "label_dict = corpus.make_label_dictionary()\n",
    "\n",
    "# document_embeddings = DocumentPoolEmbeddings([WordEmbeddings('glove')])\n",
    "# word_embeddings = [WordEmbeddings('glove'),\n",
    "#                    # comment in flair embeddings for state-of-the-art results\n",
    "#                    # FlairEmbeddings('news-forward'),\n",
    "#                    # FlairEmbeddings('news-backward'),\n",
    "#                    ]\n",
    "# document_embeddings = DocumentPoolEmbeddings([WordEmbeddings('glove'),\n",
    "# #                    FlairEmbeddings('news-forward'),\n",
    "# #                    FlairEmbeddings('news-backward'),\n",
    "# #                                              ])\n",
    "# document_embeddings(sent)\n",
    "\n",
    "document_embeddings = DocumentRNNEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n",
    "\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "trainer.train('resources/taggers/test',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              anneal_factor=0.5,\n",
    "              patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b145efca60b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdocument_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDocumentRNNEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreproject_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreproject_words_dimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_dictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m trainer.train('resources/taggers/ag_news',\n",
      "\u001b[0;31mNameError\u001b[0m: name 'label_dict' is not defined"
     ]
    }
   ],
   "source": [
    "document_embeddings = DocumentRNNEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n",
    "\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "trainer.train('resources/taggers/ag_news',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              anneal_factor=0.5,\n",
    "              patience=5)\n",
    "\n",
    "# from flair.visual.training_curves import Plotter\n",
    "# plotter = Plotter()\n",
    "# plotter.plot_weights('resources/taggers/ag_news/weights.txt')\n",
    "\n",
    "\n",
    "# classifier = TextClassifier.load('resources/taggers/ag_news/final-model.pt')\n",
    "# sentence = Sentence('France is the current world cup winner.')\n",
    "# classifier.predict(sentence)\n",
    "# print(sentence.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence: \"The grass is green .\"   [− Tokens: 5]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.embeddings import WordEmbeddings\n",
    "\n",
    "sent = Sentence('The grass is green.', use_tokenizer=True)\n",
    "# glove = WordEmbeddings('glove')\n",
    "# glove.embed(sent)\n",
    "\n",
    "# # now check out the embedded tokens.\n",
    "# for token in sentence:\n",
    "#     print(token)\n",
    "#     print(token.embedding)\n",
    "\n",
    "# [x.embedding.shape for x in sent]\n",
    "# [x for x in sent]\n",
    "sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "df = pd.read_csv(\"/workspace/flair/data/Full-Economic-News-DFE-839861.csv\", encoding='ISO-8859-1')\n",
    "documents = df.loc[:1000, \"text\"].tolist()\n",
    "# docs[0]\n",
    "# documents = [\n",
    "#     \"Human machine interface for lab abc computer applications\",\n",
    "#     \"A survey of user opinion of computer system response time\",\n",
    "#     \"The EPS user interface management system\",\n",
    "#     \"System and human system engineering testing of EPS\",\n",
    "#     \"Relation of user perceived response time to error measurement\",\n",
    "#     \"The generation of random binary unordered trees\",\n",
    "#     \"The intersection graph of paths in trees\",\n",
    "#     \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "#     \"Graph minors A survey\",\n",
    "# ]\n",
    "\n",
    "from pprint import pprint  # pretty-printer\n",
    "from collections import defaultdict\n",
    "\n",
    "# remove common words and tokenize\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [\n",
    "    [word for word in remove_stopwords(d).lower().split()]\n",
    "    for d in documents\n",
    "]\n",
    "\n",
    "# remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [\n",
    "    [token for token in text if frequency[token] > 1]\n",
    "    for text in texts\n",
    "]\n",
    "\n",
    "# pprint(texts)\n",
    "\n",
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "# dictionary.save('/tmp/deerwester.dict')  # store the dictionary, for future reference\n",
    "# print(dictionary)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "# corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"--\" + 0.009*\"the\" + 0.007*\"u.s.\" + 0.006*\"said\" + 0.005*\"federal\" + 0.004*\"mr.\" + 0.004*\"market\" + 0.004*\"fed\" + 0.004*\"economic\" + 0.003*\"new\"'),\n",
       " (1,\n",
       "  '0.011*\"the\" + 0.009*\"new\" + 0.007*\"market\" + 0.007*\"said\" + 0.007*\"u.s.\" + 0.005*\"mr.\" + 0.005*\"stock\" + 0.005*\"fed\" + 0.005*\"economic\" + 0.005*\"--\"'),\n",
       " (2,\n",
       "  '0.008*\"--\" + 0.007*\"new\" + 0.007*\"u.s.\" + 0.005*\"market\" + 0.005*\"said\" + 0.005*\"the\" + 0.004*\"stock\" + 0.003*\"economic\" + 0.003*\"billion\" + 0.003*\"rates\"'),\n",
       " (3,\n",
       "  '0.009*\"the\" + 0.007*\"new\" + 0.006*\"stock\" + 0.006*\"u.s.\" + 0.006*\"prices\" + 0.005*\"said\" + 0.004*\"market\" + 0.004*\"economic\" + 0.004*\"--\" + 0.004*\"companies\"'),\n",
       " (4,\n",
       "  '0.011*\"--\" + 0.009*\"the\" + 0.007*\"said\" + 0.007*\"new\" + 0.007*\"u.s.\" + 0.006*\"market\" + 0.006*\"stock\" + 0.005*\"economic\" + 0.004*\"rates\" + 0.004*\"york\"'),\n",
       " (5,\n",
       "  '0.010*\"the\" + 0.009*\"u.s.\" + 0.008*\"--\" + 0.006*\"said\" + 0.006*\"stock\" + 0.006*\"investors\" + 0.005*\"market\" + 0.005*\"new\" + 0.005*\"year\" + 0.004*\"rates\"'),\n",
       " (6,\n",
       "  '0.009*\"said\" + 0.009*\"the\" + 0.006*\"dollar\" + 0.006*\"u.s.\" + 0.006*\"economic\" + 0.005*\"rates\" + 0.005*\"--\" + 0.005*\"market\" + 0.005*\"investors\" + 0.005*\"stock\"'),\n",
       " (7,\n",
       "  '0.010*\"the\" + 0.010*\"said\" + 0.008*\"u.s.\" + 0.008*\"--\" + 0.006*\"prices\" + 0.006*\"economy\" + 0.005*\"rate\" + 0.004*\"new\" + 0.004*\"inflation\" + 0.004*\"rates\"'),\n",
       " (8,\n",
       "  '0.008*\"the\" + 0.007*\"--\" + 0.007*\"new\" + 0.006*\"said\" + 0.005*\"u.s.\" + 0.005*\"stock\" + 0.005*\"prices\" + 0.005*\"market\" + 0.004*\"economic\" + 0.004*\"economy\"'),\n",
       " (9,\n",
       "  '0.015*\"the\" + 0.007*\"new\" + 0.007*\"--\" + 0.006*\"billion\" + 0.005*\"market\" + 0.004*\"u.s.\" + 0.004*\"said\" + 0.003*\"year\" + 0.003*\"federal\" + 0.003*\"dollar\"')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim import models\n",
    "\n",
    "tfidf = models.TfidfModel(corpus)  # step 1 -- initialize a model\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "# for doc in corpus_tfidf:\n",
    "#     print(doc)\n",
    "\n",
    "# lsi_model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=3)  # initialize an LSI transformation\n",
    "model = models.LdaModel(corpus, id2word=dictionary, num_topics=10)\n",
    "corpus_lsi = model[corpus_tfidf]  # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\n",
    "model.print_topics(10)\n",
    "\n",
    "# both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
    "# for doc, as_text in zip(corpus_lsi, documents):\n",
    "#     print(doc, as_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog dog 1.0\n",
      "dog cat 0.80168545\n",
      "dog banana 0.24327646\n",
      "cat dog 0.80168545\n",
      "cat cat 1.0\n",
      "cat banana 0.2815437\n",
      "banana dog 0.24327646\n",
      "banana cat 0.2815437\n",
      "banana banana 1.0\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")  # make sure to use larger model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AXP American 0.041551307\n",
      "AXP Express 0.011548235\n",
      "AXP Company -0.17779963\n"
     ]
    }
   ],
   "source": [
    "# F,Ford Motor Company,NYQ,Auto Manufacturers - Major,USA,,,\n",
    "# MSFT,Microsoft Corporation,NMS,Business Software & Services,USA,,,\n",
    "tks = nlp(\"AXP American Express Company\")\n",
    "# t1, t2 = nlp(\"F Ford\")\n",
    "\n",
    "# for token in tks:\n",
    "#     print(token.text, token.has_vector, token.vector_norm, token.is_oov)\n",
    "\n",
    "# for token1 in tks:\n",
    "token1 = tks[0]\n",
    "for token2 in tks[1:]:\n",
    "    print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Walt Disney Company\n"
     ]
    }
   ],
   "source": [
    "t1 = \"The Walt Disney Company\"\n",
    "t2 = \"The Walt Disney\"\n",
    "d1 = nlp(t1)\n",
    "d2 = nlp(t2)\n",
    "\n",
    "# type(doc.ents[0])\n",
    "# displacy.render(doc, style=\"ent\")\n",
    "# s = doc.ents[0]\n",
    "# s.vector\n",
    "# d1.similarity(d2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "t = 'NEW YORK -- Yields on most certificates of deposit offered by major banks dropped more than a tenth of a percentage point in the latest week, reflecting the overall decline in short-term interest rates. On small-denomination, or \"consumer,\" CDs sold directly by banks, the average yield on six-month deposits fell to 5.49% from 5.62% in the week ended yesterday, according to an 18-bank survey by Banxquote Money Markets, a Wilmington, Del., information service.</br></br>On three-month \"consumer\" deposits, the average yield sank to 5.29% from 5.42% the week before, according to Banxquote. Two banks in the Banxquote survey, Citibank in New York and CoreStates in Pennsylvania, are paying less than 5% on threemonth small-denomination CDs.</br></br>Declines were somewhat smaller on five-year consumer CDs, which eased to 7.37% from 7.45%, Banxquote said.</br></br>Yields on three-month and six-month Treasury bills sold at Monday\\'s auction plummeted more than a fifth of a percentage point from the previous week, to 5.46% and 5.63%, respectively.'\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "displacy.render(doc, style=\"ent\")\n",
    "doc[0]\n",
    "doc[0].text, doc[0].ent_type_, doc[0].ent_kb_id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-11 14:01:10,984 Reading data from /root/.flair/datasets/trec_6\n",
      "2020-03-11 14:01:10,987 Train: /root/.flair/datasets/trec_6/train.txt\n",
      "2020-03-11 14:01:11,001 Dev: None\n",
      "2020-03-11 14:01:11,007 Test: /root/.flair/datasets/trec_6/test.txt\n",
      "2020-03-11 14:01:11,864 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4907/4907 [00:00<00:00, 21493.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-11 14:01:12,101 [b'LOC', b'ENTY', b'HUM', b'NUM', b'ABBR', b'DESC']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 417659 into shape (400000,100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6b62388bbc60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTREC_6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mlabel_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_label_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m word_embeddings = [WordEmbeddings('glove'),\n\u001b[0m\u001b[1;32m     10\u001b[0m                    \u001b[0;31m# comment in flair embeddings for state-of-the-art results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                    \u001b[0mFlairEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'news-forward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/flair/flair/embeddings.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embeddings, field)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             self.precomputed_word_embeddings = gensim.models.KeyedVectors.load(\n\u001b[0;32m--> 325\u001b[0;31m                 \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             )\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m   1538\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1540\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWordEmbeddingsKeyedVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFastTextKeyedVectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'compatible_hash'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseKeyedVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36m_load_specials\u001b[0;34m(self, fname, mmap, compress, subname)\u001b[0m\n\u001b[1;32m    467\u001b[0m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mignore_deprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0;32m--> 447\u001b[0;31m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    448\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    740\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m             \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 417659 into shape (400000,100)"
     ]
    }
   ],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import TREC_6\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentRNNEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "corpus = TREC_6()\n",
    "label_dict = corpus.make_label_dictionary()\n",
    "word_embeddings = [WordEmbeddings('glove'),\n",
    "                   # comment in flair embeddings for state-of-the-art results\n",
    "                   FlairEmbeddings('news-forward'),\n",
    "                   # FlairEmbeddings('news-backward'),\n",
    "                   ]\n",
    "document_embeddings = DocumentRNNEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "\n",
    "trainer.train('resources/taggers/ag_news',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              anneal_factor=0.5,\n",
    "              patience=5,\n",
    "              max_epochs=150)\n",
    "\n",
    "from flair.visual.training_curves import Plotter\n",
    "plotter = Plotter()\n",
    "plotter.plot_weights('resources/taggers/ag_news/weights.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "\n",
    "classifier = TextClassifier.load('en-sentiment')\n",
    "sentence = Sentence('Flair is pretty neat!')\n",
    "\n",
    "classifier.predict(sentence)\n",
    "# print sentence with predicted labels\n",
    "\n",
    "print('Sentence above is: ', sentence.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-11 13:28:36,083 loading file /root/.flair/models/en-chunk-conll2000-fast-v0.4.pt\n",
      "iTV <S-NP> Will <B-VP> Boost <E-VP> Apple <B-NP> http:\\/\\/t.co\\/8dup4cQc08 <I-NP> $ <I-NP> #APPLE <E-NP>\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# t = 'NEW YORK -- Yields on most certificates of deposit offered by major banks dropped more than a tenth of a percentage point in the latest week, reflecting the overall decline in short-term interest rates.</br></br>On small-denomination, or \"consumer,\" CDs sold directly by banks, the average yield on six-month deposits fell to 5.49% from 5.62% in the week ended yesterday, according to an 18-bank survey by Banxquote Money Markets, a Wilmington, Del., information service.</br></br>On three-month \"consumer\" deposits, the average yield sank to 5.29% from 5.42% the week before, according to Banxquote. Two banks in the Banxquote survey, Citibank in New York and CoreStates in Pennsylvania, are paying less than 5% on threemonth small-denomination CDs.</br></br>Declines were somewhat smaller on five-year consumer CDs, which eased to 7.37% from 7.45%, Banxquote said.</br></br>Yields on three-month and six-month Treasury bills sold at Monday\\'s auction plummeted more than a fifth of a percentage point from the previous week, to 5.46% and 5.63%, respectively.'\n",
    "t = \"iTV Will Boost Apple http:\\/\\/t.co\\/8dup4cQc08 $AAPL #APPLE\"\n",
    "t = t.replace(\"AAPL\", \"\")\n",
    "\n",
    "sentence = Sentence(t)\n",
    "# [x for x in sentence]\n",
    "\n",
    "tagger = SequenceTagger.load(\"chunk-fast\")\n",
    "# tagger = SequenceTagger.load(\"ner-ontonotes-fast\")\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# print(\"Analysing %s\" % sentence)\n",
    "print(sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<NP-span (1): \"iTV\">,\n",
       " <VP-span (2,3): \"Will Boost\">,\n",
       " <NP-span (4,5,6,7): \"Apple http:\\/\\/t.co\\/8dup4cQc08 $ #APPLE\">]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.get_spans('np')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-28 06:10:34,080 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models-v0.4/release-chunk-fast-0/en-chunk-conll2000-fast-v0.4.pt not found in cache, downloading to /tmp/tmpdhmzfc8v\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75233247/75233247 [00:37<00:00, 2018218.86B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-28 06:11:13,724 copying /tmp/tmpdhmzfc8v to cache at /root/.flair/models/en-chunk-conll2000-fast-v0.4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-28 06:11:13,899 removing temp file /tmp/tmpdhmzfc8v\n",
      "2020-02-28 06:11:13,930 loading file /root/.flair/models/en-chunk-conll2000-fast-v0.4.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NEW <B-NP> YORK <E-NP> -- Yields <S-NP> on <S-PP> most <B-NP> certificates <E-NP> of <S-PP> deposit <S-NP> offered <S-VP> by <S-PP> major <B-NP> banks <E-NP> dropped <S-VP> more <B-NP> than <I-NP> a <I-NP> tenth <E-NP> of <S-PP> a <B-NP> percentage <I-NP> point <E-NP> in <S-PP> the <B-NP> latest <I-NP> week, <E-NP> reflecting <S-VP> the <B-NP> overall <I-NP> decline <E-NP> in <S-PP> short-term <B-NP> interest <I-NP> rates.</br></br>On <E-NP> small-denomination, or \"consumer,\" <B-NP> CDs <E-NP> sold <S-VP> directly <S-ADVP> by <S-PP> banks, <S-NP> the <B-NP> average <I-NP> yield <E-NP> on <S-PP> six-month <B-NP> deposits <E-NP> fell <S-VP> to <S-PP> 5.49% <S-NP> from <S-PP> 5.62% <S-NP> in <S-PP> the <B-NP> week <E-NP> ended <S-VP> yesterday, <S-NP> according <S-PP> to <S-PP> an <B-NP> 18-bank <I-NP> survey <E-NP> by <S-PP> Banxquote <B-NP> Money <I-NP> Markets, <E-NP> a <B-NP> Wilmington, <I-NP> Del., <I-NP> information <I-NP> service.</br></br>On <I-NP> three-month <I-NP> \"consumer\" <E-NP> deposits, the <B-NP> average <I-NP> yield <E-NP> sank <S-VP> to <S-PP> 5.29% <S-NP> from <S-PP> 5.42% <S-NP> the <B-NP> week <E-NP> before, according <S-PP> to <S-PP> Banxquote. <B-NP> Two <I-NP> banks <E-NP> in <S-PP> the <B-NP> Banxquote <I-NP> survey, <I-NP> Citibank <E-NP> in <S-PP> New <B-NP> York <E-NP> and CoreStates <S-NP> in <S-PP> Pennsylvania, <S-NP> are <B-VP> paying <E-VP> less <B-NP> than <I-NP> 5% <E-NP> on <S-PP> threemonth <B-NP> small-denomination <I-NP> CDs.</br></br>Declines <E-NP> were <S-VP> somewhat <B-ADJP> smaller <E-ADJP> on <S-PP> five-year <B-NP> consumer <I-NP> CDs, <E-NP> which <S-NP> eased <S-VP> to <S-PP> 7.37% <S-NP> from <S-PP> 7.45%, <B-NP> Banxquote <I-NP> said.</br></br>Yields <E-NP> on <S-PP> three-month <B-NP> and <I-NP> six-month <I-NP> Treasury <I-NP> bills <E-NP> sold <S-VP> at <S-PP> Monday\\'s <B-NP> auction <E-NP> plummeted <S-VP> more <B-NP> than <I-NP> a <I-NP> fifth <E-NP> of <S-PP> a <B-NP> percentage <I-NP> point <E-NP> from <S-PP> the <B-NP> previous <I-NP> week, <E-NP> to <S-PP> 5.46% <S-NP> and 5.63%, <S-NP> respectively.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "t = 'NEW YORK -- Yields on most certificates of deposit offered by major banks dropped more than a tenth of a percentage point in the latest week, reflecting the overall decline in short-term interest rates.</br></br>On small-denomination, or \"consumer,\" CDs sold directly by banks, the average yield on six-month deposits fell to 5.49% from 5.62% in the week ended yesterday, according to an 18-bank survey by Banxquote Money Markets, a Wilmington, Del., information service.</br></br>On three-month \"consumer\" deposits, the average yield sank to 5.29% from 5.42% the week before, according to Banxquote. Two banks in the Banxquote survey, Citibank in New York and CoreStates in Pennsylvania, are paying less than 5% on threemonth small-denomination CDs.</br></br>Declines were somewhat smaller on five-year consumer CDs, which eased to 7.37% from 7.45%, Banxquote said.</br></br>Yields on three-month and six-month Treasury bills sold at Monday\\'s auction plummeted more than a fifth of a percentage point from the previous week, to 5.46% and 5.63%, respectively.'\n",
    "\n",
    "tagger = SequenceTagger.load(\"chunk-fast\")\n",
    "sentence = Sentence(t)\n",
    "tagger.predict(sentence)\n",
    "sentence.to_tagged_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in sentence.get_spans('ner'):\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dow Falls 45.95, Late GM Surge Stanches Losses'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/workspace/flair/data/Full-Economic-News-DFE-839861.csv\", encoding='ISO-8859-1')\n",
    "# df.head\n",
    "# _unit_id,_golden,_unit_state,_trusted_judgments,_last_judgment_at,positivity,positivity:confidence,relevance,relevance:confidence,articleid,date,headline,positivity_gold,relevance_gold,text\n",
    "# a, b, c = df.loc[0, [\"date\", \"headline\", \"text\"]]\n",
    "# df.loc[12, [\"date\", \"headline\", \"text\"]][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NEW YORK -- Yields on most certificates of deposit offered by major banks dropped more than a tenth of a percentage point in the latest week, reflecting the overall decline in short-term interest rates.</br></br>On small-denomination, or \"consumer,\" CDs sold directly by banks, the average yield on six-month deposits fell to 5.49% from 5.62% in the week ended yesterday, according to an 18-bank survey by Banxquote Money Markets, a Wilmington, Del., information service.</br></br>On three-month \"consumer\" deposits, the average yield sank to 5.29% from 5.42% the week before, according to Banxquote. Two banks in the Banxquote survey, Citibank in New York and CoreStates in Pennsylvania, are paying less than 5% on threemonth small-denomination CDs.</br></br>Declines were somewhat smaller on five-year consumer CDs, which eased to 7.37% from 7.45%, Banxquote said.</br></br>Yields on three-month and six-month Treasury bills sold at Monday\\'s auction plummeted more than a fifth of a percentage point from the previous week, to 5.46% and 5.63%, respectively.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.loc[10, [\"date\", \"headline\", \"text\"]][2]\n",
    "docs = df.loc[:10, \"text\"].tolist()\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-26 13:30:40,873 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models-v0.4/NER-conll03-english/en-ner-conll03-v0.4.pt not found in cache, downloading to /tmp/tmp4xhqochp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 432197603/432197603 [47:36<00:00, 151296.68B/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-26 14:18:18,476 copying /tmp/tmp4xhqochp to cache at /root/.flair/models/en-ner-conll03-v0.4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-26 14:18:19,110 removing temp file /tmp/tmp4xhqochp\n",
      "2020-02-26 14:18:19,240 loading file /root/.flair/models/en-ner-conll03-v0.4.pt\n",
      "Analysing Sentence: \"George Washington went to Washington .\" - 6 Tokens\n",
      "\n",
      "The following NER tags are found: \n",
      "\n",
      "George <B-PER> Washington <E-PER> went to Washington <S-LOC> .\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "tagger: SequenceTagger = SequenceTagger.load(\"ner\")\n",
    "\n",
    "sentence: Sentence = Sentence(\"George Washington went to Washington .\")\n",
    "tagger.predict(sentence)\n",
    "\n",
    "print(\"Analysing %s\" % sentence)\n",
    "print(\"\\nThe following NER tags are found: \\n\")\n",
    "print(sentence.to_tagged_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
