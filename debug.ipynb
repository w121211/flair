{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/flair\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/flair\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# abcnews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/workspace/flair/data/abcnews-date-text.csv\")\n",
    "df = df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import Sentence, WordEmbeddings, FlairEmbeddings, DocumentPoolEmbeddings, DocumentRNNEmbeddings\n",
    "\n",
    "document_embeddings = DocumentPoolEmbeddings([WordEmbeddings('glove')])\n",
    "\n",
    "X = []\n",
    "for i, d in enumerate(df[\"headline_text\"]):\n",
    "    sent = Sentence(d, use_tokenizer=True)\n",
    "    document_embeddings.embed(sent)\n",
    "    x = sent.get_embedding()\n",
    "    x = x.detach().numpy()\n",
    "    X.append(x)\n",
    "X = np.stack(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby(by=\"publish_date\").agg([\"count\"])\n",
    "groups = df.groupby(by=\"publish_date\")\n",
    "groups.groups.keys()\n",
    "keys = [20030219, 20030220, 20030221, 20030222, 20030223, 20030224]\n",
    "# groups[20030219]\n",
    "# groups.get_group(20030219)\n",
    "d0 = groups.get_group(keys[0])\n",
    "d1 = groups.get_group(keys[1])\n",
    "d2 = groups.get_group(keys[1])\n",
    "\n",
    "# for i, d in enumerate(df[\"headline_text\"]):\n",
    "#     print(i, d)\n",
    "#     break\n",
    "\n",
    "X0 = X[d0.index[0]:d0.index[-1]+1]\n",
    "X1 = X[d1.index[0]:d1.index[-1]+1]\n",
    "# X2 = X[d2.index[0]:d2.index[-1]+1]\n",
    "\n",
    "# X0_1 = X[d0.index[0]:d1.index[-1]+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "      <th>c0</th>\n",
       "      <th>c1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date headline_text    c0    c1\n",
       "          count         count count count\n",
       "y                                        \n",
       "0             4             4     3     3\n",
       "1            37            37    15    20\n",
       "2            10            10     9     9\n",
       "3            38            38     8    15\n",
       "4            14            14    14    14\n",
       "5            14            14     8     9\n",
       "6             6             6     5     6\n",
       "7            22            22    15    15\n",
       "8             4             4     4     4\n",
       "9             3             3     1     1\n",
       "10           15            15     9    10\n",
       "11            6             6     4     4\n",
       "12           11            11    11    11\n",
       "13           12            12     8    10\n",
       "14           19            19     3     5\n",
       "15           12            12     7     8\n",
       "16            4             4     3     4\n",
       "17            1             1     1     1\n",
       "18           11            11    11    11\n",
       "19            3             3     2     2\n",
       "20           26            26    18    19\n",
       "21           16            16     4     4\n",
       "22            9             9     9     9\n",
       "23           21            21     1     8\n",
       "24            5             5     4     4\n",
       "25           10            10     2     3\n",
       "26            7             7     1     2\n",
       "27            3             3     3     3\n",
       "28           14            14    14    14\n",
       "29            1             1     1     1\n",
       "31            2             2     0     1\n",
       "32           10            10     0     1\n",
       "34            6             6     0     2\n",
       "36           22            22     0     2\n",
       "38            1             1     0     0\n",
       "39            2             2     0     0\n",
       "40            4             4     0     1\n",
       "41           31            31     0    10\n",
       "43            1             1     0     0\n",
       "47            6             6     0     2\n",
       "48            2             2     0     0\n",
       "49            3             3     0     2"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN, AffinityPropagation, MeanShift, AgglomerativeClustering\n",
    "\n",
    "clustering = KMeans(n_clusters=50, random_state=0).fit(X0)\n",
    "# clustering.predict(X1)\n",
    "y = np.concatenate([c0.labels_, clustering.predict(X1)])\n",
    "# c1 = KMeans(n_clusters=45, random_state=0).fit(X1)\n",
    "\n",
    "# # z = np.zeros(df.shape[0], dtype=np.int32)\n",
    "# z = np.empty((df.shape[0],)) * np.nan\n",
    "# z[:c0.labels_.shape[0]] = c0.labels_\n",
    "# df[\"c0\"] = z\n",
    "\n",
    "# z = np.empty((df.shape[0],)) * np.nan\n",
    "# z[:c1.labels_.shape[0]] = c1.labels_\n",
    "# df[\"c1\"] = z\n",
    "# df.head(100)\n",
    "\n",
    "_df = df[0:len(y)]\n",
    "_df[\"y\"] = y\n",
    "# _df.loc[_df[\"y\"] == 20, [\"headline_text\", \"y\"]]\n",
    "_df.groupby(\"y\").agg([\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>c0</th>\n",
       "      <th>c1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>call for ethanol blend fuel to go ahead</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>health minister backs organ and tissue storage</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>last minute call hands alinghi big lead</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>more women urged to become councillors</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>omodei to stay in politics</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>rain eases wheatbelt water woes</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>uni to continue tree disease study</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>vowles to retire at end of season</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>brisbane sparkies head for 10 day strike</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>bungle leaves doctor waiting to practise</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>council general manager to step down</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      headline_text    c0   c1\n",
       "22          call for ethanol blend fuel to go ahead  18.0  2.0\n",
       "69   health minister backs organ and tissue storage   5.0  2.0\n",
       "85          last minute call hands alinghi big lead  22.0  2.0\n",
       "100          more women urged to become councillors  20.0  2.0\n",
       "117                      omodei to stay in politics  28.0  2.0\n",
       "145                 rain eases wheatbelt water woes  17.0  2.0\n",
       "184              uni to continue tree disease study  20.0  2.0\n",
       "189               vowles to retire at end of season  28.0  2.0\n",
       "215        brisbane sparkies head for 10 day strike   NaN  2.0\n",
       "219        bungle leaves doctor waiting to practise   NaN  2.0\n",
       "238            council general manager to step down   NaN  2.0"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = df.groupby(\"c0\")\n",
    "df.loc[df[\"c1\"] == 2, [\"headline_text\", \"c0\", \"c1\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 0, 0], dtype=int16)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3])\n",
    "z = np.zeros(5, dtype=np.int16)\n",
    "z[:a.shape[0]] = a\n",
    "# z\n",
    "# a.shape\n",
    "# a.shape[0]\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 82   0 120  91  91  65  26 109  21 115 212 148 234 225  30 107  73 105\n",
      "   3  75   0 153  49 148 213  36  40 132 114 260  15 229 229 159  33  90\n",
      "   9 109 135  93 159  47  23 168  42 219  76  76 156  24 170  84  84 120\n",
      " 254 254 205  70  24 193  45 159 151 154  15 212  12 115  79  36  48 169\n",
      " 214  90 161   9 120  52 136  62 205  60 185 130 168 100  91  52   8 203\n",
      "   8 166 176 121 244  76 120 171 152  30 132 260  73 150 135 115 183 203\n",
      "  87  25 213  32  39 190  10 207 240 135 122 132 181  41  19 106  19 217\n",
      " 126 226 259 120 128 154  24 203  80  18 235  16   6  91 234  82   6  81\n",
      " 145 129  61 178 223  47 237 111 206 118 119  55  56  81 100 137  64  90\n",
      " 155  12  62 171  45  34  49  16 165 152  41 219  28  41 122  22  15  70\n",
      " 106 204 124 132  25 105  16 260 139  29  26 215 227  58 186 201 141  14\n",
      " 257 100 162 114 176 170 175  79  96  82 124 234 157  13 148  96  93  29\n",
      "  64  10 141 256 117 183 120  27 252 101 201  67 251  40  46  19  46 200\n",
      "   0 131  81  97 114  76  41 118 160  51  77 189  92 112  95  95  17 200\n",
      " 227 206 141 100 145 178  28 250  38 134 101  60  41 154 103  13 150 222\n",
      " 100  50  99  67  98  71 259 133  79  33  76 154  47 116 202 245  15 165\n",
      "  26 211 112 112 147  32 136 135   4 120 161  47 218 168  56  42 241  61\n",
      "  26 252  12 251 205 205  72 115  18  14 166  34   8  52 236 223 222 106\n",
      " 106   8 189 179  15 141  79  29 196 188 211 180 152 164 252 228  49 103\n",
      " 126 117  15 135 115 100 144 190  28  71 115 115 141 105  75 235  38 157\n",
      " 131 134  63 211   0  57  16  24 164  16  16 121  31  70 136 135 129  18\n",
      " 211 164   6 157 116 170  72 184 213 126  61  27 144 233  17  61  20  15\n",
      " 153 159 159   4 256 256   4  74  66  55 195 175 120  27 242  24 106 185\n",
      "   0  55  62 116   5 178  98  34 165 163 183 183  85 155  38  21  59 183\n",
      " 183 177  26  12  33 115 212 181 193 136 171 128 178 106 173  42 185  76\n",
      " 176   5 222 215 202 202   2  48 162 212  56  54  54  66 137 226  59 159\n",
      " 213  78  57 246  61 109  12  10  98 158 146 146 177 191  11 199 199 128\n",
      " 185  12 148 180  75  25  22 251  71  74  71  28 133 234  68 217 142  27\n",
      " 180  35 115  19  84  80  80 130 117  76 201 253 258  51 254   5 127  39\n",
      "  95  46 118  27  12  84  12  33 135  47  13  32  58   7  89  65 100 193\n",
      " 130 130 169  81 118 165  63  15 249 148  44  52 133  87 120 161 238 240\n",
      " 147  12  79 100  43 114  61   5 217  30  43 220  30  23  52   8   8 118\n",
      " 236 236   8   8   8 106 100 141 110 244 120   2  30 167  73  73 141  19\n",
      "  42  26 103 216  22 159 178  23  39 121 207 102  42  29 122 106  62 217\n",
      " 217  50  71 154 186 195  21  63 112  63 245   8  57   6 112 219  15 131\n",
      "  98 120 150 185 219 208  17  95  95 107  95  30  37  33  59 102  61  78\n",
      " 110 176  52 228   4 125  54 136  54  12  37  50  76  49  49  25  18 219\n",
      " 230  94  94  94 224 226 148 113 183 183 183  85  19  59 193  80  76  77\n",
      " 123  27 123 150  29   0 215 182 135 108   5  79  55 260  54 139 220 137\n",
      "  85  41  42 248   9  23 118 113  11  29  19  86  99  64 192 176 153  35\n",
      "   4 172 227  54 112 123  43  83 108 160  67  55 153  32 127   1 175 240\n",
      "  21 221 221 205  68 119  61   7  44  69 201 183   2  11   4  85  67 175\n",
      " 247  27 239 151   3 194  41  82  50  43   6 199  71 168 168   2  71 154\n",
      " 172   8  22  22 187 226 162   2  27 107 160  81 249 252   9  83 241  89\n",
      "  52  79 168 230 109  52 154 225  86 174  75 115  28  88  76 158  54 183\n",
      "  52 115 197  54 252  54 205  59 213 154 215 144  61  14   8 162  50 173\n",
      "  54 134  77   5 130 112 108 155  98 184  21  63  88  32   1 138   1  35\n",
      " 167 178  40  34  70  92  29 227 210 154 234  12 112  22 194 129 154 101\n",
      " 242   7   1 225  33 257  29 154 209  29 154 121  82 148  97  70 249  70\n",
      " 109  26 148  45 255   7  44 194   3 104   8  52   8   8 158 189 122 153\n",
      "   0  23  89 116  78 113 112 257 115 190  26  30  64  19  12  11 166  83\n",
      " 188  24 112 148 107  53  96 252  21  18 243 125 125 107 220  99 107 140\n",
      " 140  20  17 181  23  43 125  29 134   1 115 183 228 155 155 155 215 230\n",
      "  17  61  29 201  21 218  90  46   8   5 111 149  25  31  97 257  69  71\n",
      " 143 108  13  41 157 108  56 120  50 213  53  61   6 257 152  80  30 232\n",
      " 231 219  59 157  16  50 260 126 198 204]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>former star koen licks cats</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>cat lovers march for romes sacred strays</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>cats claw dogs pies swoop on blues</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>fake crocs scare off flamingos</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>tourists cry foul over spilt milk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                headline_text  y\n",
       "735               former star koen licks cats  1\n",
       "842  cat lovers march for romes sacred strays  1\n",
       "844        cats claw dogs pies swoop on blues  1\n",
       "866            fake crocs scare off flamingos  1\n",
       "945         tourists cry foul over spilt milk  1"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN, AffinityPropagation, MeanShift, AgglomerativeClustering\n",
    "\n",
    "# clustering = KMeans(n_clusters=100, random_state=0).fit(X)\n",
    "# clustering = DBSCAN(eps=0.11, min_samples=1, metric=\"cosine\").fit(X)\n",
    "# clustering = AffinityPropagation(max_iter=2000, convergence_iter=15,).fit(X)\n",
    "# clustering = MeanShift(max_iter=3000).fit(X)\n",
    "# clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=3).fit(X)\n",
    "\n",
    "print(clustering.labels_)\n",
    "df[\"y\"] = list(clustering.labels_)\n",
    "\n",
    "# print(kmeans.predict([[0, 0], [12, 3]]))\n",
    "# print(kmeans.cluster_centers_)\n",
    "# df[\"label\"] = kmeans.labels_\n",
    "# df[df[\"label\"] == 0]\n",
    "\n",
    "# len(df)\n",
    "df.loc[df[\"y\"] == 1, [\"headline_text\", \"y\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 0, 0], dtype=int16)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"a\" : [1,2,3,4,5],\"b\" : [6, 7, 8, 9,10]})\n",
    "# df[\"d\"] = None\n",
    "# df.loc[0, \"d\"] = 1\n",
    "# df[\"a\"].tolist()\n",
    "# df.loc[0:2, \"c\"] = [0,1]\n",
    "# df[\"c\"] = None\n",
    "# df.loc[0:2, \"c\"] = [0,1,2]\n",
    "df[\"c\"] = np.zeros(5).astype(np.int16)\n",
    "df\n",
    "# df[\"c\"].apply(pd.to_numeric)\n",
    "a = np.zeros(5).astype(np.int16)\n",
    "a[0:3] = np.array([1,2,3])\n",
    "a\n",
    "\n",
    "# groups = df.groupby(by=\"publish_date\")\n",
    "# groups.iloc['20030219']\n",
    "# .agg([\"count\"])\n",
    "# for name, group in groups:\n",
    "#     print(name)\n",
    "#     for i, row in group.iterrows():\n",
    "#         print(row['headline_text'])\n",
    "#         col = row['column']\n",
    "#         column_type = row['column_type']\n",
    "#         is_null = 'NOT NULL' if row['is_null'] == 'NO' else ''\n",
    "#         print('\\t{} {} {},'.format(col, column_type, is_null))\n",
    "\n",
    "# gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'contributors': None,\n",
      " 'coordinates': None,\n",
      " 'created_at': 'Tue Oct 14 13:39:49 +0000 2014',\n",
      " 'entities': {'hashtags': [],\n",
      "              'symbols': [{'indices': [0, 4], 'text': 'GDP'},\n",
      "                          {'indices': [42, 47], 'text': 'TSLA'},\n",
      "                          {'indices': [48, 53], 'text': 'TWTR'},\n",
      "                          {'indices': [54, 56], 'text': 'C'},\n",
      "                          {'indices': [57, 61], 'text': 'GDP'},\n",
      "                          {'indices': [62, 67], 'text': 'AMZN'},\n",
      "                          {'indices': [68, 73], 'text': 'DRYS'},\n",
      "                          {'indices': [74, 77], 'text': 'VZ'}],\n",
      "              'trends': [],\n",
      "              'urls': [{'display_url': 'twitter.com/CBOE/status/52…',\n",
      "                        'expanded_url': 'https://twitter.com/CBOE/status/522018942626594816',\n",
      "                        'indices': [81, 104],\n",
      "                        'url': 'https://t.co/7Q6GX3gUuS'},\n",
      "                       {'display_url': 'quantpost.com/list/?symbol=G…',\n",
      "                        'expanded_url': 'https://quantpost.com/list/?symbol=GDP',\n",
      "                        'indices': [117, 140],\n",
      "                        'url': 'https://t.co/3mM5DHJBo5'}],\n",
      "              'user_mentions': [{'id': 16827489,\n",
      "                                 'id_str': '16827489',\n",
      "                                 'indices': [28, 33],\n",
      "                                 'name': 'CBOE',\n",
      "                                 'screen_name': 'CBOE'}]},\n",
      " 'favorite_count': 0,\n",
      " 'favorited': False,\n",
      " 'filter_level': 'medium',\n",
      " 'geo': None,\n",
      " 'id': 522018944454914050,\n",
      " 'id_str': '522018944454914050',\n",
      " 'in_reply_to_screen_name': None,\n",
      " 'in_reply_to_status_id': None,\n",
      " 'in_reply_to_status_id_str': None,\n",
      " 'in_reply_to_user_id': None,\n",
      " 'in_reply_to_user_id_str': None,\n",
      " 'lang': 'en',\n",
      " 'place': None,\n",
      " 'possibly_sensitive': False,\n",
      " 'retweet_count': 0,\n",
      " 'retweeted': False,\n",
      " 'source': '<a href=\"https://quantpost.com\" rel=\"nofollow\">Quantpost '\n",
      "           'Herald</a>',\n",
      " 'text': '$GDP News: \"Actives on open @CBOE: $ AAPL $TSLA $TWTR $C $GDP $AMZN '\n",
      "         '$DRYS $VZ …\" https://t.co/7Q6GX3gUuS Board view: '\n",
      "         'https://t.co/3mM5DHJBo5',\n",
      " 'timestamp_ms': '1413293989664',\n",
      " 'truncated': False,\n",
      " 'user': {'contributors_enabled': False,\n",
      "          'created_at': 'Tue Oct 01 16:13:12 +0000 2013',\n",
      "          'default_profile': False,\n",
      "          'default_profile_image': False,\n",
      "          'description': 'Option Traders: A real-time machine and human '\n",
      "                         'monitor of relevant Basic Materials Sector news and '\n",
      "                         'market events. See also '\n",
      "                         '@quantpost,@qp_basic,@qp_financial,...',\n",
      "          'favourites_count': 0,\n",
      "          'follow_request_sent': None,\n",
      "          'followers_count': 109,\n",
      "          'following': None,\n",
      "          'friends_count': 186,\n",
      "          'geo_enabled': False,\n",
      "          'id': 1923902360,\n",
      "          'id_str': '1923902360',\n",
      "          'is_translator': False,\n",
      "          'lang': 'en',\n",
      "          'listed_count': 5,\n",
      "          'location': '',\n",
      "          'name': 'Quantpost Basic Mats',\n",
      "          'notifications': None,\n",
      "          'profile_background_color': 'FFFFFF',\n",
      "          'profile_background_image_url': 'http://pbs.twimg.com/profile_background_images/378800000086180953/8400c43a4d4f8926bc84ddb373b8a88c.png',\n",
      "          'profile_background_image_url_https': 'https://pbs.twimg.com/profile_background_images/378800000086180953/8400c43a4d4f8926bc84ddb373b8a88c.png',\n",
      "          'profile_background_tile': False,\n",
      "          'profile_banner_url': 'https://pbs.twimg.com/profile_banners/1923902360/1411214979',\n",
      "          'profile_image_url': 'http://pbs.twimg.com/profile_images/513298893434417153/dy9NrfhO_normal.png',\n",
      "          'profile_image_url_https': 'https://pbs.twimg.com/profile_images/513298893434417153/dy9NrfhO_normal.png',\n",
      "          'profile_link_color': '009999',\n",
      "          'profile_sidebar_border_color': '000000',\n",
      "          'profile_sidebar_fill_color': 'DDEEF6',\n",
      "          'profile_text_color': '333333',\n",
      "          'profile_use_background_image': False,\n",
      "          'protected': False,\n",
      "          'screen_name': 'QP_Basic',\n",
      "          'statuses_count': 4916,\n",
      "          'time_zone': 'Mountain Time (US & Canada)',\n",
      "          'url': 'http://quantpost.com',\n",
      "          'utc_offset': -21600,\n",
      "          'verified': False}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import os \n",
    "import json \n",
    "directory = \"/workspace/stocknet-dataset/tweet/raw\"\n",
    "\n",
    "for ticker in os.listdir(directory):\n",
    "    p = os.path.join(directory, ticker)\n",
    "    if os.path.isfile(p):\n",
    "        continue\n",
    "    for fn in os.listdir(p):\n",
    "        with open(os.path.join(directory, ticker, fn)) as f:\n",
    "            for i, l in enumerate(f.readlines()):\n",
    "                print(i)\n",
    "                j = json.loads(l)\n",
    "#                 print(j[\"text\"])\n",
    "                pprint(j)\n",
    "#                 print(l)\n",
    "        break\n",
    "    break\n",
    "\n",
    "# [f for f in listdir(p)]\n",
    "# for root, dirnames, filenames in os.walk(d):\n",
    "#     for fileanme in filenames:\n",
    "#         p = os.path.join(root, dirname, filename)\n",
    "#         print(p)\n",
    "#         with open(p) as f:\n",
    "#             pass\n",
    "#         break\n",
    "        \n",
    "#     for filename in filenames:\n",
    "#         if \".DS_Store\" in filename:\n",
    "#             continue\n",
    "#         path = os.path.join(root, filename)\n",
    "#         print(root, filename)\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yields on CDs Fell in the Latest Week</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Morning Brief: White House Seeks to Limit ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Banking Bill Negotiators Set Compromise --- Pl...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Manager's Journal: Sniffing Out Drug Abusers I...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Currency Trading: Dollar Remains in Tight Rang...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stocks Fall Again; BofA, Alcoa Slide</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bank of Montreal, Royal Bank Profits Rose in 2...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Battle Over Medical Costs Isn't Over</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sequenom to Buy Gemini Genomics In Stock Accord</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>U.S. Dollar Falls Against Most Currencies; Dec...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Dow Falls 45.95, Late GM Surge Stanches Losses</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             headline  label\n",
       "0               Yields on CDs Fell in the Latest Week      0\n",
       "1   The Morning Brief: White House Seeks to Limit ...      1\n",
       "2   Banking Bill Negotiators Set Compromise --- Pl...      2\n",
       "3   Manager's Journal: Sniffing Out Drug Abusers I...      3\n",
       "4   Currency Trading: Dollar Remains in Tight Rang...      4\n",
       "5                Stocks Fall Again; BofA, Alcoa Slide      5\n",
       "6   Bank of Montreal, Royal Bank Profits Rose in 2...      6\n",
       "7                Battle Over Medical Costs Isn't Over      7\n",
       "8     Sequenom to Buy Gemini Genomics In Stock Accord      8\n",
       "9   U.S. Dollar Falls Against Most Currencies; Dec...      9\n",
       "10     Dow Falls 45.95, Late GM Surge Stanches Losses     10"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "df = pd.read_csv(\"/workspace/flair/data/Full-Economic-News-DFE-839861.csv\", encoding='ISO-8859-1')\n",
    "# documents = df.loc[:100, \"text\"].tolist()\n",
    "df = df.loc[:10, [\"headline\"]]\n",
    "df['label'] = [i for i in range(0,11)]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaa']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# documents\n",
    "df = pd.DataFrame({\"d\" : [\"aaa\" ,\"bbb\", \"ccc\"]})\n",
    "df['label'] = [1,1,0]\n",
    "df[df.label == 1]\n",
    "\n",
    "[x for x in df[\"d\"].head(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-30 15:11:26,618 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models-v0.4/NER-conll03-english/en-ner-conll03-v0.4.pt not found in cache, downloading to /tmp/tmpo8_dribe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 432197603/432197603 [02:45<00:00, 2604231.44B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-30 15:14:14,169 copying /tmp/tmpo8_dribe to cache at /root/.flair/models/en-ner-conll03-v0.4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-30 15:14:15,801 removing temp file /tmp/tmpo8_dribe\n",
      "2020-03-30 15:14:15,961 loading file /root/.flair/models/en-ner-conll03-v0.4.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Sentence: \"I love Berlin .\"   [− Tokens: 4  − Token-Labels: \"I love Berlin <S-LOC> .\"]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# make a sentence\n",
    "sentence = Sentence('I love Berlin .')\n",
    "\n",
    "# load the NER tagger\n",
    "tagger = SequenceTagger.load('ner')\n",
    "\n",
    "# run NER over sentence\n",
    "tagger.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 2)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "X = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]])\n",
    "# X.shape\n",
    "# kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "# print(kmeans.labels_)\n",
    "# print(kmeans.predict([[0, 0], [12, 3]]))\n",
    "# print(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>positivity</th>\n",
       "      <th>positivity:confidence</th>\n",
       "      <th>relevance</th>\n",
       "      <th>relevance:confidence</th>\n",
       "      <th>articleid</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>positivity_gold</th>\n",
       "      <th>relevance_gold</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>842613460</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/4/15 23:15</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6783</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>wsj_905654974</td>\n",
       "      <td>11/23/11</td>\n",
       "      <td>Stocks Fall Again; BofA, Alcoa Slide</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stocks declined, as investors weighed slower-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>842613511</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/5/15 7:26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>1.0</td>\n",
       "      <td>wsj_905646207</td>\n",
       "      <td>11/23/11</td>\n",
       "      <td>Indian Rupee Falls Again</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MUMBAI - The Indian rupee fell against the U.S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "5   842613460    False   finalized                   3     12/4/15 23:15   \n",
       "56  842613511    False   finalized                   3      12/5/15 7:26   \n",
       "\n",
       "    positivity  positivity:confidence relevance  relevance:confidence  \\\n",
       "5          3.0                 0.6783       yes                   1.0   \n",
       "56         NaN                    NaN        no                   1.0   \n",
       "\n",
       "        articleid      date                              headline  \\\n",
       "5   wsj_905654974  11/23/11  Stocks Fall Again; BofA, Alcoa Slide   \n",
       "56  wsj_905646207  11/23/11              Indian Rupee Falls Again   \n",
       "\n",
       "    positivity_gold  relevance_gold  \\\n",
       "5               NaN             NaN   \n",
       "56              NaN             NaN   \n",
       "\n",
       "                                                 text  \n",
       "5   Stocks declined, as investors weighed slower-t...  \n",
       "56  MUMBAI - The Indian rupee fell against the U.S...  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/workspace/flair/data/Full-Economic-News-DFE-839861.csv\", encoding='ISO-8859-1')\n",
    "df = df.head(100)\n",
    "# [d for d in df[\"headline\"]]\n",
    "# df.columns\n",
    "g = df.groupby([\"date\"])\n",
    "g.get_group('11/23/11')\n",
    "# s = g.size()\n",
    "# s[s > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/workspace/flair/data/abcnews-date-text.csv\")\n",
    "df = df.head(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "publish_date\n",
       "20030219    198\n",
       "20030220    250\n",
       "20030221    250\n",
       "20030222    126\n",
       "20030223    136\n",
       "20030224    250\n",
       "20030225    250\n",
       "20030226    250\n",
       "20030227    221\n",
       "20030228     69\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = df.groupby([\"publish_date\"])\n",
    "g.size()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from flair.embeddings import Sentence, WordEmbeddings, FlairEmbeddings, DocumentPoolEmbeddings, DocumentRNNEmbeddings\n",
    "\n",
    "df = pd.read_csv(\"/workspace/flair/data/Full-Economic-News-DFE-839861.csv\", encoding='ISO-8859-1')\n",
    "df = df.head(100)\n",
    "\n",
    "document_embeddings = DocumentPoolEmbeddings([WordEmbeddings('glove')])\n",
    "\n",
    "X = []\n",
    "for d in df[\"headline\"]:\n",
    "    sent = Sentence(d, use_tokenizer=True)\n",
    "    document_embeddings.embed(sent)\n",
    "    x = sent.get_embedding()\n",
    "    x = x.detach().numpy()\n",
    "#     print(x)\n",
    "    X.append(x)\n",
    "\n",
    "X = np.stack(X)\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(X)\n",
    "# print(len(kmeans.labels_))\n",
    "df[\"y\"] = list(kmeans.labels_)\n",
    "# print(kmeans.predict([[0, 0], [12, 3]]))\n",
    "# print(kmeans.cluster_centers_)\n",
    "# df[\"label\"] = kmeans.labels_\n",
    "# df[df[\"label\"] == 0]\n",
    "\n",
    "# len(df)\n",
    "df.loc[df[\"y\"] == 1, [\"headline\", \"y\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Currency Trading: Dollar Remains in Tight Rang...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>U.S. Dollar Falls Against Most Currencies; Dec...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Dollar Declines as Players Take Profits From R...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Bond Prices Tumble as Dollar's Plunge Prompts ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Tuesday's markets: Prices of bonds, stocks dro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Retailers Stock Up on Caution</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Some Stocks Had Big Fourth-Quarter Gains Despi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Bond Prices Are Little Changed In Slow Trading...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Futures Markets: Treasury Bond Contracts End N...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>AMR, Southwest Profits Surge Even as Fuel Pric...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Use of stock-options contracts climbs as inves...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Asian Shares Mixed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Treasurys Prices Continue to Climb on Weakness...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Stocks Maintain Recent Gains, Giving Investors...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             headline  y\n",
       "4   Currency Trading: Dollar Remains in Tight Rang...  1\n",
       "9   U.S. Dollar Falls Against Most Currencies; Dec...  1\n",
       "17  Dollar Declines as Players Take Profits From R...  1\n",
       "26  Bond Prices Tumble as Dollar's Plunge Prompts ...  1\n",
       "35  Tuesday's markets: Prices of bonds, stocks dro...  1\n",
       "47                      Retailers Stock Up on Caution  1\n",
       "48  Some Stocks Had Big Fourth-Quarter Gains Despi...  1\n",
       "52  Bond Prices Are Little Changed In Slow Trading...  1\n",
       "60  Futures Markets: Treasury Bond Contracts End N...  1\n",
       "62  AMR, Southwest Profits Surge Even as Fuel Pric...  1\n",
       "80  Use of stock-options contracts climbs as inves...  1\n",
       "82                                 Asian Shares Mixed  1\n",
       "87  Treasurys Prices Continue to Climb on Weakness...  1\n",
       "91  Stocks Maintain Recent Gains, Giving Investors...  1"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.stack(X)\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(X)\n",
    "# print(len(kmeans.labels_))\n",
    "df[\"y\"] = list(kmeans.labels_)\n",
    "# print(kmeans.predict([[0, 0], [12, 3]]))\n",
    "# print(kmeans.cluster_centers_)\n",
    "# df[\"label\"] = kmeans.labels_\n",
    "# df[df[\"label\"] == 0]\n",
    "\n",
    "# len(df)\n",
    "df.loc[df[\"y\"] == 1, [\"headline\", \"y\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 9, 1, 0, 2, 8, 3, 5, 6, 2, 4]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from flair.data import Sentence\n",
    "from flair.embeddings import Sentence, WordEmbeddings, FlairEmbeddings, DocumentPoolEmbeddings, DocumentRNNEmbeddings\n",
    "\n",
    "sentence = Sentence('this is one. this is two. this is three.', use_tokenizer=True)\n",
    "# sentence.tokens\n",
    "# now check out the embedded tokens.\n",
    "# for token in sentence:\n",
    "#     print(token)\n",
    "#     print(token.embedding)\n",
    "\n",
    "# document_embeddings = DocumentPoolEmbeddings([WordEmbeddings('glove'),\n",
    "#                                               FlairEmbeddings('news-backward'),\n",
    "#                                               FlairEmbeddings('news-forward')])\n",
    "document_embeddings = DocumentPoolEmbeddings([WordEmbeddings('glove')])\n",
    "document_embeddings.embed(sentence)\n",
    "# print(sentence.get_embedding())\n",
    "\n",
    "# from flair.embeddings import WordEmbeddings, DocumentRNNEmbeddings\n",
    "\n",
    "# glove_embedding = WordEmbeddings('glove')\n",
    "# document_embeddings = DocumentRNNEmbeddings([glove_embedding])\n",
    "# sentence = Sentence('The grass is green . And the sky is blue .')\n",
    "# document_embeddings.embed(sentence)\n",
    "# print(sentence.get_embedding())\n",
    "\n",
    "# document_lstm_embeddings = DocumentRNNEmbeddings([WordEmbeddings('glove')], rnn_type='LSTM')\n",
    "# document_embeddings.embed(sentence)\n",
    "# print(sentence.get_embedding())\n",
    "\n",
    "x = sentence.get_embedding()\n",
    "x.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "\n",
    "# sent = Sentence('The grass is green.', use_tokenizer=True)\n",
    "sent = Sentence('The grass is green.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-b0c153bf060d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m                    FlairEmbeddings('news-backward'),])\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mdocument_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mregistered\u001b[0m \u001b[0mhooks\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatter\u001b[0m \u001b[0msilently\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mthem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from flair.data import segtok_tokenizer, Sentence\n",
    "from flair.datasets import ClassificationDataset\n",
    "from flair.datasets import ClassificationCorpus\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentPoolEmbeddings, DocumentRNNEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "# ds = ClassificationDataset(\n",
    "#     path_to_file='/workspace/flair/data/news/train.txt',\n",
    "#     in_memory=True,\n",
    "# )\n",
    "# ds[0]\n",
    "\n",
    "# corpus = ClassificationCorpus(data_folder='/workspace/flair/data/news', tokenizer=segtok_tokenizer, in_memory=True)\n",
    "# corpus.train[0]\n",
    "# label_dict = corpus.make_label_dictionary()\n",
    "\n",
    "# document_embeddings = DocumentPoolEmbeddings([WordEmbeddings('glove')])\n",
    "word_embeddings = [WordEmbeddings('glove'),\n",
    "                   # comment in flair embeddings for state-of-the-art results\n",
    "                   # FlairEmbeddings('news-forward'),\n",
    "                   # FlairEmbeddings('news-backward'),\n",
    "                   ]\n",
    "document_embeddings = DocumentPoolEmbeddings([WordEmbeddings('glove'),\n",
    "                   FlairEmbeddings('news-forward'),\n",
    "                   FlairEmbeddings('news-backward'),])\n",
    "\n",
    "document_embeddings(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings = DocumentRNNEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n",
    "\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "trainer.train('resources/taggers/ag_news',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              anneal_factor=0.5,\n",
    "              patience=5)\n",
    "\n",
    "# from flair.visual.training_curves import Plotter\n",
    "# plotter = Plotter()\n",
    "# plotter.plot_weights('resources/taggers/ag_news/weights.txt')\n",
    "\n",
    "\n",
    "# classifier = TextClassifier.load('resources/taggers/ag_news/final-model.pt')\n",
    "# sentence = Sentence('France is the current world cup winner.')\n",
    "# classifier.predict(sentence)\n",
    "# print(sentence.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence: \"The grass is green .\"   [− Tokens: 5]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.embeddings import WordEmbeddings\n",
    "\n",
    "sent = Sentence('The grass is green.', use_tokenizer=True)\n",
    "# glove = WordEmbeddings('glove')\n",
    "# glove.embed(sent)\n",
    "\n",
    "# # now check out the embedded tokens.\n",
    "# for token in sentence:\n",
    "#     print(token)\n",
    "#     print(token.embedding)\n",
    "\n",
    "# [x.embedding.shape for x in sent]\n",
    "# [x for x in sent]\n",
    "sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "df = pd.read_csv(\"/workspace/flair/data/Full-Economic-News-DFE-839861.csv\", encoding='ISO-8859-1')\n",
    "documents = df.loc[:1000, \"text\"].tolist()\n",
    "# docs[0]\n",
    "# documents = [\n",
    "#     \"Human machine interface for lab abc computer applications\",\n",
    "#     \"A survey of user opinion of computer system response time\",\n",
    "#     \"The EPS user interface management system\",\n",
    "#     \"System and human system engineering testing of EPS\",\n",
    "#     \"Relation of user perceived response time to error measurement\",\n",
    "#     \"The generation of random binary unordered trees\",\n",
    "#     \"The intersection graph of paths in trees\",\n",
    "#     \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "#     \"Graph minors A survey\",\n",
    "# ]\n",
    "\n",
    "from pprint import pprint  # pretty-printer\n",
    "from collections import defaultdict\n",
    "\n",
    "# remove common words and tokenize\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [\n",
    "    [word for word in remove_stopwords(d).lower().split()]\n",
    "    for d in documents\n",
    "]\n",
    "\n",
    "# remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [\n",
    "    [token for token in text if frequency[token] > 1]\n",
    "    for text in texts\n",
    "]\n",
    "\n",
    "# pprint(texts)\n",
    "\n",
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "# dictionary.save('/tmp/deerwester.dict')  # store the dictionary, for future reference\n",
    "# print(dictionary)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "# corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"--\" + 0.009*\"the\" + 0.007*\"u.s.\" + 0.006*\"said\" + 0.005*\"federal\" + 0.004*\"mr.\" + 0.004*\"market\" + 0.004*\"fed\" + 0.004*\"economic\" + 0.003*\"new\"'),\n",
       " (1,\n",
       "  '0.011*\"the\" + 0.009*\"new\" + 0.007*\"market\" + 0.007*\"said\" + 0.007*\"u.s.\" + 0.005*\"mr.\" + 0.005*\"stock\" + 0.005*\"fed\" + 0.005*\"economic\" + 0.005*\"--\"'),\n",
       " (2,\n",
       "  '0.008*\"--\" + 0.007*\"new\" + 0.007*\"u.s.\" + 0.005*\"market\" + 0.005*\"said\" + 0.005*\"the\" + 0.004*\"stock\" + 0.003*\"economic\" + 0.003*\"billion\" + 0.003*\"rates\"'),\n",
       " (3,\n",
       "  '0.009*\"the\" + 0.007*\"new\" + 0.006*\"stock\" + 0.006*\"u.s.\" + 0.006*\"prices\" + 0.005*\"said\" + 0.004*\"market\" + 0.004*\"economic\" + 0.004*\"--\" + 0.004*\"companies\"'),\n",
       " (4,\n",
       "  '0.011*\"--\" + 0.009*\"the\" + 0.007*\"said\" + 0.007*\"new\" + 0.007*\"u.s.\" + 0.006*\"market\" + 0.006*\"stock\" + 0.005*\"economic\" + 0.004*\"rates\" + 0.004*\"york\"'),\n",
       " (5,\n",
       "  '0.010*\"the\" + 0.009*\"u.s.\" + 0.008*\"--\" + 0.006*\"said\" + 0.006*\"stock\" + 0.006*\"investors\" + 0.005*\"market\" + 0.005*\"new\" + 0.005*\"year\" + 0.004*\"rates\"'),\n",
       " (6,\n",
       "  '0.009*\"said\" + 0.009*\"the\" + 0.006*\"dollar\" + 0.006*\"u.s.\" + 0.006*\"economic\" + 0.005*\"rates\" + 0.005*\"--\" + 0.005*\"market\" + 0.005*\"investors\" + 0.005*\"stock\"'),\n",
       " (7,\n",
       "  '0.010*\"the\" + 0.010*\"said\" + 0.008*\"u.s.\" + 0.008*\"--\" + 0.006*\"prices\" + 0.006*\"economy\" + 0.005*\"rate\" + 0.004*\"new\" + 0.004*\"inflation\" + 0.004*\"rates\"'),\n",
       " (8,\n",
       "  '0.008*\"the\" + 0.007*\"--\" + 0.007*\"new\" + 0.006*\"said\" + 0.005*\"u.s.\" + 0.005*\"stock\" + 0.005*\"prices\" + 0.005*\"market\" + 0.004*\"economic\" + 0.004*\"economy\"'),\n",
       " (9,\n",
       "  '0.015*\"the\" + 0.007*\"new\" + 0.007*\"--\" + 0.006*\"billion\" + 0.005*\"market\" + 0.004*\"u.s.\" + 0.004*\"said\" + 0.003*\"year\" + 0.003*\"federal\" + 0.003*\"dollar\"')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim import models\n",
    "\n",
    "tfidf = models.TfidfModel(corpus)  # step 1 -- initialize a model\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "# for doc in corpus_tfidf:\n",
    "#     print(doc)\n",
    "\n",
    "# lsi_model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=3)  # initialize an LSI transformation\n",
    "model = models.LdaModel(corpus, id2word=dictionary, num_topics=10)\n",
    "corpus_lsi = model[corpus_tfidf]  # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\n",
    "model.print_topics(10)\n",
    "\n",
    "# both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
    "# for doc, as_text in zip(corpus_lsi, documents):\n",
    "#     print(doc, as_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog dog 1.0\n",
      "dog cat 0.80168545\n",
      "dog banana 0.24327646\n",
      "cat dog 0.80168545\n",
      "cat cat 1.0\n",
      "cat banana 0.2815437\n",
      "banana dog 0.24327646\n",
      "banana cat 0.2815437\n",
      "banana banana 1.0\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")  # make sure to use larger model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AXP American 0.041551307\n",
      "AXP Express 0.011548235\n",
      "AXP Company -0.17779963\n"
     ]
    }
   ],
   "source": [
    "# F,Ford Motor Company,NYQ,Auto Manufacturers - Major,USA,,,\n",
    "# MSFT,Microsoft Corporation,NMS,Business Software & Services,USA,,,\n",
    "tks = nlp(\"AXP American Express Company\")\n",
    "# t1, t2 = nlp(\"F Ford\")\n",
    "\n",
    "# for token in tks:\n",
    "#     print(token.text, token.has_vector, token.vector_norm, token.is_oov)\n",
    "\n",
    "# for token1 in tks:\n",
    "token1 = tks[0]\n",
    "for token2 in tks[1:]:\n",
    "    print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Walt Disney Company\n"
     ]
    }
   ],
   "source": [
    "t1 = \"The Walt Disney Company\"\n",
    "t2 = \"The Walt Disney\"\n",
    "d1 = nlp(t1)\n",
    "d2 = nlp(t2)\n",
    "\n",
    "# type(doc.ents[0])\n",
    "# displacy.render(doc, style=\"ent\")\n",
    "# s = doc.ents[0]\n",
    "# s.vector\n",
    "# d1.similarity(d2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "t = 'NEW YORK -- Yields on most certificates of deposit offered by major banks dropped more than a tenth of a percentage point in the latest week, reflecting the overall decline in short-term interest rates. On small-denomination, or \"consumer,\" CDs sold directly by banks, the average yield on six-month deposits fell to 5.49% from 5.62% in the week ended yesterday, according to an 18-bank survey by Banxquote Money Markets, a Wilmington, Del., information service.</br></br>On three-month \"consumer\" deposits, the average yield sank to 5.29% from 5.42% the week before, according to Banxquote. Two banks in the Banxquote survey, Citibank in New York and CoreStates in Pennsylvania, are paying less than 5% on threemonth small-denomination CDs.</br></br>Declines were somewhat smaller on five-year consumer CDs, which eased to 7.37% from 7.45%, Banxquote said.</br></br>Yields on three-month and six-month Treasury bills sold at Monday\\'s auction plummeted more than a fifth of a percentage point from the previous week, to 5.46% and 5.63%, respectively.'\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "displacy.render(doc, style=\"ent\")\n",
    "doc[0]\n",
    "doc[0].text, doc[0].ent_type_, doc[0].ent_kb_id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-11 14:01:10,984 Reading data from /root/.flair/datasets/trec_6\n",
      "2020-03-11 14:01:10,987 Train: /root/.flair/datasets/trec_6/train.txt\n",
      "2020-03-11 14:01:11,001 Dev: None\n",
      "2020-03-11 14:01:11,007 Test: /root/.flair/datasets/trec_6/test.txt\n",
      "2020-03-11 14:01:11,864 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4907/4907 [00:00<00:00, 21493.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-11 14:01:12,101 [b'LOC', b'ENTY', b'HUM', b'NUM', b'ABBR', b'DESC']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 417659 into shape (400000,100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6b62388bbc60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTREC_6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mlabel_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_label_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m word_embeddings = [WordEmbeddings('glove'),\n\u001b[0m\u001b[1;32m     10\u001b[0m                    \u001b[0;31m# comment in flair embeddings for state-of-the-art results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                    \u001b[0mFlairEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'news-forward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/flair/flair/embeddings.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embeddings, field)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             self.precomputed_word_embeddings = gensim.models.KeyedVectors.load(\n\u001b[0;32m--> 325\u001b[0;31m                 \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             )\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m   1538\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1540\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWordEmbeddingsKeyedVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFastTextKeyedVectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'compatible_hash'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseKeyedVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36m_load_specials\u001b[0;34m(self, fname, mmap, compress, subname)\u001b[0m\n\u001b[1;32m    467\u001b[0m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mignore_deprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0;32m--> 447\u001b[0;31m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    448\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    740\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m             \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 417659 into shape (400000,100)"
     ]
    }
   ],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import TREC_6\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentRNNEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "corpus = TREC_6()\n",
    "label_dict = corpus.make_label_dictionary()\n",
    "word_embeddings = [WordEmbeddings('glove'),\n",
    "                   # comment in flair embeddings for state-of-the-art results\n",
    "                   FlairEmbeddings('news-forward'),\n",
    "                   # FlairEmbeddings('news-backward'),\n",
    "                   ]\n",
    "document_embeddings = DocumentRNNEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "\n",
    "trainer.train('resources/taggers/ag_news',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              anneal_factor=0.5,\n",
    "              patience=5,\n",
    "              max_epochs=150)\n",
    "\n",
    "from flair.visual.training_curves import Plotter\n",
    "plotter = Plotter()\n",
    "plotter.plot_weights('resources/taggers/ag_news/weights.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "\n",
    "classifier = TextClassifier.load('en-sentiment')\n",
    "sentence = Sentence('Flair is pretty neat!')\n",
    "\n",
    "classifier.predict(sentence)\n",
    "# print sentence with predicted labels\n",
    "\n",
    "print('Sentence above is: ', sentence.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-11 13:28:36,083 loading file /root/.flair/models/en-chunk-conll2000-fast-v0.4.pt\n",
      "iTV <S-NP> Will <B-VP> Boost <E-VP> Apple <B-NP> http:\\/\\/t.co\\/8dup4cQc08 <I-NP> $ <I-NP> #APPLE <E-NP>\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# t = 'NEW YORK -- Yields on most certificates of deposit offered by major banks dropped more than a tenth of a percentage point in the latest week, reflecting the overall decline in short-term interest rates.</br></br>On small-denomination, or \"consumer,\" CDs sold directly by banks, the average yield on six-month deposits fell to 5.49% from 5.62% in the week ended yesterday, according to an 18-bank survey by Banxquote Money Markets, a Wilmington, Del., information service.</br></br>On three-month \"consumer\" deposits, the average yield sank to 5.29% from 5.42% the week before, according to Banxquote. Two banks in the Banxquote survey, Citibank in New York and CoreStates in Pennsylvania, are paying less than 5% on threemonth small-denomination CDs.</br></br>Declines were somewhat smaller on five-year consumer CDs, which eased to 7.37% from 7.45%, Banxquote said.</br></br>Yields on three-month and six-month Treasury bills sold at Monday\\'s auction plummeted more than a fifth of a percentage point from the previous week, to 5.46% and 5.63%, respectively.'\n",
    "t = \"iTV Will Boost Apple http:\\/\\/t.co\\/8dup4cQc08 $AAPL #APPLE\"\n",
    "t = t.replace(\"AAPL\", \"\")\n",
    "\n",
    "sentence = Sentence(t)\n",
    "# [x for x in sentence]\n",
    "\n",
    "tagger = SequenceTagger.load(\"chunk-fast\")\n",
    "# tagger = SequenceTagger.load(\"ner-ontonotes-fast\")\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# print(\"Analysing %s\" % sentence)\n",
    "print(sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<NP-span (1): \"iTV\">,\n",
       " <VP-span (2,3): \"Will Boost\">,\n",
       " <NP-span (4,5,6,7): \"Apple http:\\/\\/t.co\\/8dup4cQc08 $ #APPLE\">]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.get_spans('np')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-28 06:10:34,080 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models-v0.4/release-chunk-fast-0/en-chunk-conll2000-fast-v0.4.pt not found in cache, downloading to /tmp/tmpdhmzfc8v\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75233247/75233247 [00:37<00:00, 2018218.86B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-28 06:11:13,724 copying /tmp/tmpdhmzfc8v to cache at /root/.flair/models/en-chunk-conll2000-fast-v0.4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-28 06:11:13,899 removing temp file /tmp/tmpdhmzfc8v\n",
      "2020-02-28 06:11:13,930 loading file /root/.flair/models/en-chunk-conll2000-fast-v0.4.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NEW <B-NP> YORK <E-NP> -- Yields <S-NP> on <S-PP> most <B-NP> certificates <E-NP> of <S-PP> deposit <S-NP> offered <S-VP> by <S-PP> major <B-NP> banks <E-NP> dropped <S-VP> more <B-NP> than <I-NP> a <I-NP> tenth <E-NP> of <S-PP> a <B-NP> percentage <I-NP> point <E-NP> in <S-PP> the <B-NP> latest <I-NP> week, <E-NP> reflecting <S-VP> the <B-NP> overall <I-NP> decline <E-NP> in <S-PP> short-term <B-NP> interest <I-NP> rates.</br></br>On <E-NP> small-denomination, or \"consumer,\" <B-NP> CDs <E-NP> sold <S-VP> directly <S-ADVP> by <S-PP> banks, <S-NP> the <B-NP> average <I-NP> yield <E-NP> on <S-PP> six-month <B-NP> deposits <E-NP> fell <S-VP> to <S-PP> 5.49% <S-NP> from <S-PP> 5.62% <S-NP> in <S-PP> the <B-NP> week <E-NP> ended <S-VP> yesterday, <S-NP> according <S-PP> to <S-PP> an <B-NP> 18-bank <I-NP> survey <E-NP> by <S-PP> Banxquote <B-NP> Money <I-NP> Markets, <E-NP> a <B-NP> Wilmington, <I-NP> Del., <I-NP> information <I-NP> service.</br></br>On <I-NP> three-month <I-NP> \"consumer\" <E-NP> deposits, the <B-NP> average <I-NP> yield <E-NP> sank <S-VP> to <S-PP> 5.29% <S-NP> from <S-PP> 5.42% <S-NP> the <B-NP> week <E-NP> before, according <S-PP> to <S-PP> Banxquote. <B-NP> Two <I-NP> banks <E-NP> in <S-PP> the <B-NP> Banxquote <I-NP> survey, <I-NP> Citibank <E-NP> in <S-PP> New <B-NP> York <E-NP> and CoreStates <S-NP> in <S-PP> Pennsylvania, <S-NP> are <B-VP> paying <E-VP> less <B-NP> than <I-NP> 5% <E-NP> on <S-PP> threemonth <B-NP> small-denomination <I-NP> CDs.</br></br>Declines <E-NP> were <S-VP> somewhat <B-ADJP> smaller <E-ADJP> on <S-PP> five-year <B-NP> consumer <I-NP> CDs, <E-NP> which <S-NP> eased <S-VP> to <S-PP> 7.37% <S-NP> from <S-PP> 7.45%, <B-NP> Banxquote <I-NP> said.</br></br>Yields <E-NP> on <S-PP> three-month <B-NP> and <I-NP> six-month <I-NP> Treasury <I-NP> bills <E-NP> sold <S-VP> at <S-PP> Monday\\'s <B-NP> auction <E-NP> plummeted <S-VP> more <B-NP> than <I-NP> a <I-NP> fifth <E-NP> of <S-PP> a <B-NP> percentage <I-NP> point <E-NP> from <S-PP> the <B-NP> previous <I-NP> week, <E-NP> to <S-PP> 5.46% <S-NP> and 5.63%, <S-NP> respectively.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "t = 'NEW YORK -- Yields on most certificates of deposit offered by major banks dropped more than a tenth of a percentage point in the latest week, reflecting the overall decline in short-term interest rates.</br></br>On small-denomination, or \"consumer,\" CDs sold directly by banks, the average yield on six-month deposits fell to 5.49% from 5.62% in the week ended yesterday, according to an 18-bank survey by Banxquote Money Markets, a Wilmington, Del., information service.</br></br>On three-month \"consumer\" deposits, the average yield sank to 5.29% from 5.42% the week before, according to Banxquote. Two banks in the Banxquote survey, Citibank in New York and CoreStates in Pennsylvania, are paying less than 5% on threemonth small-denomination CDs.</br></br>Declines were somewhat smaller on five-year consumer CDs, which eased to 7.37% from 7.45%, Banxquote said.</br></br>Yields on three-month and six-month Treasury bills sold at Monday\\'s auction plummeted more than a fifth of a percentage point from the previous week, to 5.46% and 5.63%, respectively.'\n",
    "\n",
    "tagger = SequenceTagger.load(\"chunk-fast\")\n",
    "sentence = Sentence(t)\n",
    "tagger.predict(sentence)\n",
    "sentence.to_tagged_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in sentence.get_spans('ner'):\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dow Falls 45.95, Late GM Surge Stanches Losses'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/workspace/flair/data/Full-Economic-News-DFE-839861.csv\", encoding='ISO-8859-1')\n",
    "# df.head\n",
    "# _unit_id,_golden,_unit_state,_trusted_judgments,_last_judgment_at,positivity,positivity:confidence,relevance,relevance:confidence,articleid,date,headline,positivity_gold,relevance_gold,text\n",
    "# a, b, c = df.loc[0, [\"date\", \"headline\", \"text\"]]\n",
    "# df.loc[12, [\"date\", \"headline\", \"text\"]][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NEW YORK -- Yields on most certificates of deposit offered by major banks dropped more than a tenth of a percentage point in the latest week, reflecting the overall decline in short-term interest rates.</br></br>On small-denomination, or \"consumer,\" CDs sold directly by banks, the average yield on six-month deposits fell to 5.49% from 5.62% in the week ended yesterday, according to an 18-bank survey by Banxquote Money Markets, a Wilmington, Del., information service.</br></br>On three-month \"consumer\" deposits, the average yield sank to 5.29% from 5.42% the week before, according to Banxquote. Two banks in the Banxquote survey, Citibank in New York and CoreStates in Pennsylvania, are paying less than 5% on threemonth small-denomination CDs.</br></br>Declines were somewhat smaller on five-year consumer CDs, which eased to 7.37% from 7.45%, Banxquote said.</br></br>Yields on three-month and six-month Treasury bills sold at Monday\\'s auction plummeted more than a fifth of a percentage point from the previous week, to 5.46% and 5.63%, respectively.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.loc[10, [\"date\", \"headline\", \"text\"]][2]\n",
    "docs = df.loc[:10, \"text\"].tolist()\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-26 13:30:40,873 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models-v0.4/NER-conll03-english/en-ner-conll03-v0.4.pt not found in cache, downloading to /tmp/tmp4xhqochp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 432197603/432197603 [47:36<00:00, 151296.68B/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-26 14:18:18,476 copying /tmp/tmp4xhqochp to cache at /root/.flair/models/en-ner-conll03-v0.4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-26 14:18:19,110 removing temp file /tmp/tmp4xhqochp\n",
      "2020-02-26 14:18:19,240 loading file /root/.flair/models/en-ner-conll03-v0.4.pt\n",
      "Analysing Sentence: \"George Washington went to Washington .\" - 6 Tokens\n",
      "\n",
      "The following NER tags are found: \n",
      "\n",
      "George <B-PER> Washington <E-PER> went to Washington <S-LOC> .\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "tagger: SequenceTagger = SequenceTagger.load(\"ner\")\n",
    "\n",
    "sentence: Sentence = Sentence(\"George Washington went to Washington .\")\n",
    "tagger.predict(sentence)\n",
    "\n",
    "print(\"Analysing %s\" % sentence)\n",
    "print(\"\\nThe following NER tags are found: \\n\")\n",
    "print(sentence.to_tagged_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
